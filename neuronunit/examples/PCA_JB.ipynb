{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Python version: 3.5.2 |Continuum Analytics, Inc.| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "Collecting hdbscan\n",
      "Collecting cython>=0.27 (from hdbscan)\n",
      "  Using cached https://files.pythonhosted.org/packages/c2/34/99ced126b3f41a908d8883570a67fbf900f10eea3cfdd11e388eb8ae9aac/Cython-0.29.6-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.9 in /opt/conda/lib/python3.5/site-packages (from hdbscan) (1.12.1)\n",
      "Requirement already satisfied: scikit-learn>=0.17 in /opt/conda/lib/python3.5/site-packages (from hdbscan) (0.19.1)\n",
      "Requirement already satisfied: scipy>=0.9 in /opt/conda/lib/python3.5/site-packages (from hdbscan) (1.1.0)\n",
      "\u001b[31mcryptography 2.2.1 requires asn1crypto>=0.21.0, which is not installed.\u001b[0m\n",
      "\u001b[31mcffi 1.11.5 requires pycparser, which is not installed.\u001b[0m\n",
      "\u001b[31mallensdk 0.14.2 has requirement pandas<0.20.0,>=0.16.2, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: cython, hdbscan\n",
      "  Found existing installation: Cython 0.23.5\n",
      "\u001b[31mCannot uninstall 'Cython'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/matplotlib/__init__.py:1405: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lazyarray in /opt/conda/lib/python3.5/site-packages (0.3.2)\n",
      "Requirement already satisfied: numpy>=1.8 in /opt/conda/lib/python3.5/site-packages (from lazyarray) (1.12.1)\n",
      "\u001b[31mcffi 1.11.5 requires pycparser, which is not installed.\u001b[0m\n",
      "\u001b[31mcryptography 2.2.1 requires asn1crypto>=0.21.0, which is not installed.\u001b[0m\n",
      "\u001b[31mallensdk 0.14.2 has requirement pandas<0.20.0,>=0.16.2, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting scipy\n",
      "  Using cached https://files.pythonhosted.org/packages/f0/30/526bee2ce18c066f9ff13ba89603f6c2b96c9fd406b57a21a7ba14bf5679/scipy-1.2.1-cp35-cp35m-manylinux1_x86_64.whl\n",
      "Requirement not upgraded as not directly required: numpy>=1.8.2 in /opt/conda/lib/python3.5/site-packages (from scipy) (1.12.1)\n",
      "\u001b[31mcffi 1.11.5 requires pycparser, which is not installed.\u001b[0m\n",
      "\u001b[31mcryptography 2.2.1 requires asn1crypto>=0.21.0, which is not installed.\u001b[0m\n",
      "\u001b[31mallensdk 0.14.2 has requirement pandas<0.20.0,>=0.16.2, but you'll have pandas 0.23.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: scipy\n",
      "  Found existing installation: scipy 1.1.0\n",
      "    Uninstalling scipy-1.1.0:\n",
      "\u001b[31mCould not install packages due to an EnvironmentError: [Errno 13] Permission denied: 'trf_linear.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/pyNN/neuron/__init__.py:14: UserWarning: mpi4py not available\n",
      "  warnings.warn(\"mpi4py not available\")\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named 'neurodynex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-65e0abd697ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlmfit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneuronunit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimisation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimisation_management\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmint_generic_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minject_and_plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#scipy.stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/neuronunit/neuronunit/optimisation/optimisation_management.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mneuronunit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimisation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_neab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mneuronunit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduced\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReducedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneuronunit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimisation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_parameters\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpath_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mneuronunit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimisation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_parameters\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodelp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jovyan/neuronunit/neuronunit/optimisation/model_parameters.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mMODEL_PARAMS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneurodynex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madex_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdEx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mBAE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'neurodynex'"
     ]
    }
   ],
   "source": [
    "import sys    \n",
    "print('Running on Python version: {}'.format(sys.version))\n",
    "!pip install hdbscan \n",
    "import os\n",
    "import pickle\n",
    "import quantities as pq\n",
    "from neuronunit.tests.druckmann2013 import *\n",
    "import csv\n",
    "import urllib\n",
    "from urllib import robotparser\n",
    "import io\n",
    "!pip install lazyarray\n",
    "!pip install scipy --upgrade\n",
    "\n",
    "np.isin = np.in1d\n",
    "import dask\n",
    "db = dask.bag\n",
    "try:\n",
    "    import lazyarray\n",
    "    import lmfit\n",
    "    import scipy\n",
    "\n",
    "except:\n",
    "    !pip install scipy --upgrade\n",
    "    !pip install numpy numba --upgrade \n",
    "    !pip install lmfit \n",
    "    #!conda install rpy2\n",
    "\n",
    "import lmfit\n",
    "from neuronunit.optimisation.optimisation_management import mint_generic_model, inject_and_plot\n",
    "\n",
    "#scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    izhi_opt = pickle.load(open('protected/multi_objective_izhi.p','rb')) \n",
    "except:\n",
    "    \n",
    "    os.system('wget wget -O ncp_cell_layer_5_6.p https://osf.io/6yba2/download')\n",
    "    os.system('wget -O multi_objective_izhi https://osf.io/3vp8d/download')\n",
    "    izhi_opt = pickle.load(open('multi_objective_izhi.p','rb')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "If content is not available locally, wget the files from the Open Science Frame Work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtc0 = list(izhi_opt.values())[0]['pf'][0].dtc\n",
    "dtc1 = list(izhi_opt.values())[0]['pf'][1].dtc\n",
    "dtc2 = list(izhi_opt.values())[0]['pf'][2].dtc\n",
    "dtc_1 = list(izhi_opt.values())[0]['pf'][-1].dtc\n",
    "\n",
    "print(list(izhi_opt.values())[0]['pf'][0].dtc.scores)\n",
    "print(list(izhi_opt.values())[0]['pf'][-1].dtc.scores)\n",
    "print(list(izhi_opt.values())[0]['pf'][2].dtc.scores)\n",
    "print(list(izhi_opt.values())[0]['pf'][1].dtc.scores)\n",
    "list(izhi_opt.values())[0]['pf'][0].dtc.attrs\n",
    "dtcpop = [ p.dtc for p in list(izhi_opt.values())[0]['pf'] ]\n",
    "\n",
    "print(dtc0.rheobase)\n",
    "ir_currents = {}\n",
    "ir_currents = dtc0.rheobase\n",
    "standard = 1.5*ir_currents\n",
    "standard*=1.5\n",
    "strong = 3*ir_currents\n",
    "print(standard)\n",
    "dm_properties = {}\n",
    "print(len(dtcpop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init_tests(standard,strong):\n",
    "    tests = [AP12AmplitudeDropTest(standard), \n",
    "        AP1SSAmplitudeChangeTest(standard), \n",
    "        AP1AmplitudeTest(standard), \n",
    "        AP1WidthHalfHeightTest(standard), \n",
    "        AP1WidthPeakToTroughTest(standard), \n",
    "        AP1RateOfChangePeakToTroughTest(standard), \n",
    "        AP1AHPDepthTest(standard), \n",
    "        AP2AmplitudeTest(standard), \n",
    "        AP2WidthHalfHeightTest(standard), \n",
    "        AP2WidthPeakToTroughTest(standard), \n",
    "        AP2RateOfChangePeakToTroughTest(standard), \n",
    "        AP2AHPDepthTest(standard), \n",
    "        AP12AmplitudeChangePercentTest(standard), \n",
    "        AP12HalfWidthChangePercentTest(standard), \n",
    "        AP12RateOfChangePeakToTroughPercentChangeTest(standard), \n",
    "        AP12AHPDepthPercentChangeTest(standard), \n",
    "        AP1DelayMeanTest(standard), \n",
    "        AP1DelaySDTest(standard), \n",
    "        AP2DelayMeanTest(standard), \n",
    "        AP2DelaySDTest(standard), \n",
    "        Burst1ISIMeanTest(standard), \n",
    "        Burst1ISISDTest(standard), \n",
    "        InitialAccommodationMeanTest(standard), \n",
    "        SSAccommodationMeanTest(standard), \n",
    "        AccommodationRateToSSTest(standard), \n",
    "        AccommodationAtSSMeanTest(standard), \n",
    "        AccommodationRateMeanAtSSTest(standard), \n",
    "        ISICVTest(standard), \n",
    "        ISIMedianTest(standard), \n",
    "        ISIBurstMeanChangeTest(standard), \n",
    "        SpikeRateStrongStimTest(strong), \n",
    "        AP1DelayMeanStrongStimTest(strong), \n",
    "        AP1DelaySDStrongStimTest(strong), \n",
    "        AP2DelayMeanStrongStimTest(strong), \n",
    "        AP2DelaySDStrongStimTest(strong), \n",
    "        Burst1ISISDStrongStimTest(strong),\n",
    "        Burst1ISIMeanStrongStimTest(strong)]\n",
    "\n",
    "    AHP_list = [AP1AHPDepthTest(standard), \n",
    "        AP2AHPDepthTest(standard), \n",
    "        AP12AHPDepthPercentChangeTest(standard) ] \n",
    "    return tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtc0.vtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(list(izhi_opt.values())[0]['pf'][1].dtc.scores)\n",
    "list(izhi_opt.values())[0]['pf'][0].dtc.attrs\n",
    "\n",
    "print(dtc0.rheobase)\n",
    "ir_currents = {}\n",
    "ir_currents = dtc0.rheobase\n",
    "standard = 1.5*ir_currents\n",
    "standard*=1.5\n",
    "strong = 3*ir_currents\n",
    "print(strong)\n",
    "\n",
    "inject_and_plot(dtc0,figname='problem')\n",
    "current_amplitude = {'mean': ir_currents, 'n': 1, 'std': 0.0 * pq.pA}\n",
    "test = AP12AmplitudeChangePercentTest(current_amplitude)\n",
    "\n",
    "# = AP12AmplitudeChangePercentTest(current_amplitude)\n",
    "\n",
    "ir_currents = dtc0.rheobase\n",
    "standard = 1.5*ir_currents\n",
    "standard*=1.5\n",
    "strong = 3*ir_currents\n",
    "tests = init_tests(standard,strong)\n",
    "#print(tests)\n",
    "model = mint_generic_model(dtc0.backend)\n",
    "'''\n",
    "model.set_attrs(dtc0.attrs)\n",
    "for i, test in enumerate(tests):\n",
    "    print(i,test)\n",
    "    #model = None\n",
    "\n",
    "    print(type(model))\n",
    "    #import pdb\n",
    "    #pdb.set_trace()\n",
    "    test.generate_prediction(model)['mean']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cell_to_test_mapper(content):\n",
    "    dm_properties = {}\n",
    "    index,dtc = content\n",
    "    dm_properties[index] = []\n",
    "    ir_currents = dtc.rheobase\n",
    "    standard = 1.5*ir_currents\n",
    "    standard*=1.5\n",
    "    strong = 3*ir_currents\n",
    "    tests = init_tests(standard,strong)\n",
    "    model = None\n",
    "    model = mint_generic_model(dtc.backend)\n",
    "    \n",
    "    for i, test in enumerate(tests):\n",
    "        print(i,test)\n",
    "        model.set_attrs(dtc.attrs)\n",
    "        dm_properties[index].append(test.generate_prediction(model)['mean'])\n",
    "        print(dm_properties[index])\n",
    "    dtc.dm_properties = None\n",
    "    dtc.dm_properties = dm_properties\n",
    "    return dtc\n",
    "\n",
    "def add_dm_properties_to_cells(dtcpop):\n",
    "    \n",
    "    dm_properties = {}    \n",
    "    flatten_cells = [(index,dtc) for index,dtc in enumerate(dtcpop) ]\n",
    "    bag = db.from_sequence(flatten_cells,npartitions=8)\n",
    "\n",
    "    dtcpop = list(bag.map(cell_to_test_mapper).compute())\n",
    "    dm_properties = {}\n",
    "    for l in list_of_dics:\n",
    "        dm_properties.update(l)\n",
    "    return (dtcpop,dm_properties)\n",
    "\n",
    "dtcpop = dd_dm_properties_to_cells(dtcpop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=tests)\n",
    "df.loc[0] = [ d['mean'] for d in dm_properties ]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import pandas\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import string \n",
    "from scipy.stats import pearsonr\n",
    "#import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "from scipy.linalg import toeplitz\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "\n",
    "#import hdbscan\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "pandas.set_option('display.max_columns', None)\n",
    "pandas.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "protocol_cost = {\n",
    "    'Steady State':       1, # 1s\n",
    "    'Standard':         5*2, # 5 repetitions of 1s SS 1s stim\n",
    "    'Strong':           5*2, # 5 repetitions of 1s SS 1s strong stim\n",
    "    'Input Resistance': 2*2, # 2 levels of 1s SS 1s stim\n",
    "}\n",
    "\n",
    "prop_protocol = {\n",
    "    'AP1Amplitude': 'Standard',\n",
    "    'AP2Amplitude': 'Standard',\n",
    "    'AP12AmplitudeDrop': 'Standard',\n",
    "    'AP12AmplitudeChangePercent': 'Standard',\n",
    "    'AP1SSAmplitudeChange': 'Standard'  ,\n",
    "    \n",
    "    'AP1WidthHalfHeight': 'Standard',\n",
    "    'AP2WidthHalfHeight': 'Standard',\n",
    "    'AP12HalfWidthChangePercent': 'Standard',\n",
    "    \n",
    "    'AP1WidthPeakToTrough': 'Standard',\n",
    "    'AP2WidthPeakToTrough': 'Standard',\n",
    "    \n",
    "    'AP1RateOfChangePeakToTrough': 'Standard',\n",
    "    'AP2RateOfChangePeakToTrough': 'Standard', \n",
    "    'AP12RateOfChangePeakToTroughPercentChange': 'Standard',\n",
    "    \n",
    "    'AP1AHPDepth': 'Standard',\n",
    "    'AP2AHPDepth': 'Standard',\n",
    "    'AP12AHPDepthPercentChange': 'Standard',\n",
    "    \n",
    "    'AP1DelayMean': 'Standard',\n",
    "    'AP2DelayMean': 'Standard',\n",
    "    \n",
    "    'AP1DelaySD': 'Standard',\n",
    "    'AP2DelaySD': 'Standard',\n",
    "    \n",
    "    'AP1DelayMeanStrongStim': 'Strong',\n",
    "    'AP2DelayMeanStrongStim': 'Strong',\n",
    "    \n",
    "    'AP1DelaySDStrongStim': 'Strong',\n",
    "    'AP2DelaySDStrongStim': 'Strong',\n",
    "    \n",
    "    'Burst1ISIMean': 'Standard',\n",
    "    'Burst1ISIMeanStrongStim': 'Strong',\n",
    "    \n",
    "    'Burst1ISISD': 'Standard',\n",
    "    'Burst1ISISDStrongStim': 'Strong',\n",
    "    \n",
    "    'InitialAccommodationMean': 'Standard',\n",
    "    'SSAccommodationMean': 'Standard',\n",
    "    'AccommodationRateToSS': 'Standard',\n",
    "    'AccommodationAtSSMean': 'Standard',\n",
    "    'AccommodationRateMeanAtSS': 'Standard',\n",
    "    \n",
    "    \n",
    "    'ISIMedian': 'Standard',\n",
    "    'ISICV': 'Standard',\n",
    "    'ISIBurstMeanChange': 'Standard',\n",
    "    \n",
    "    'SpikeRateStrongStim': 'Strong',\n",
    "    \n",
    "    'InputResistance': 'Input Resistance',\n",
    "    \n",
    "    'SteadyStateAPs': 'Steady State',\n",
    "}\n",
    "\n",
    "prop_names = [\n",
    "    'AP1Amplitude',\n",
    "    'AP2Amplitude',\n",
    "    'AP12AmplitudeDrop',\n",
    "    'AP12AmplitudeChangePercent',\n",
    "    'AP1SSAmplitudeChange',  \n",
    "    \n",
    "    'AP1WidthHalfHeight',\n",
    "    'AP2WidthHalfHeight',\n",
    "    'AP12HalfWidthChangePercent',\n",
    "    \n",
    "    'AP1WidthPeakToTrough',\n",
    "    'AP2WidthPeakToTrough',\n",
    "    \n",
    "    'AP1RateOfChangePeakToTrough',\n",
    "    'AP2RateOfChangePeakToTrough',    \n",
    "    'AP12RateOfChangePeakToTroughPercentChange',\n",
    "    \n",
    "    'AP1AHPDepth',\n",
    "    'AP2AHPDepth',\n",
    "    'AP12AHPDepthPercentChange',\n",
    "    \n",
    "    'AP1DelayMean',\n",
    "    'AP2DelayMean',\n",
    "    \n",
    "    'AP1DelaySD',\n",
    "    'AP2DelaySD',\n",
    "    \n",
    "    'AP1DelayMeanStrongStim',\n",
    "    'AP2DelayMeanStrongStim',\n",
    "    \n",
    "    'AP1DelaySDStrongStim',\n",
    "    'AP2DelaySDStrongStim',\n",
    "    \n",
    "    'Burst1ISIMean',\n",
    "    'Burst1ISIMeanStrongStim',\n",
    "    \n",
    "    'Burst1ISISD',\n",
    "    'Burst1ISISDStrongStim',\n",
    "    \n",
    "    'InitialAccommodationMean',\n",
    "    'SSAccommodationMean',\n",
    "    'AccommodationRateToSS',\n",
    "    'AccommodationAtSSMean',\n",
    "    'AccommodationRateMeanAtSS',\n",
    "    \n",
    "    \n",
    "    'ISIMedian',\n",
    "    'ISICV',\n",
    "    'ISIBurstMeanChange',\n",
    "    \n",
    "    'SpikeRateStrongStim',\n",
    "    \n",
    "    'InputResistance',\n",
    "    \n",
    "    'SteadyStateAPs',\n",
    "    \n",
    "    'FrequencyPassAbove',\n",
    "    'FrequencyPassBelow',\n",
    "    \n",
    "    'RampFirstSpike',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "df['AP1Amplitude'] = 0 \n",
    "df['AP2Amplitude'] = 0\n",
    "df['AP1SSAmplitudeChange'] = 0  \n",
    "df['AP1WidthHalfHeight'] = 0 \n",
    "df['AP2WidthHalfHeight'] = 0 # .fillna(0, inplace=True)\n",
    "df['AP1WidthPeakToTrough'] = 0\n",
    "df['AP2WidthPeakToTrough'] = 0\n",
    "df['AP1RateOfChangePeakToTrough'] = 0\n",
    "df['AP2RateOfChangePeakToTrough'] = 0\n",
    "df['AP1AHPDepth'] = 0\n",
    "df['AP2AHPDepth'] = 0\n",
    "df['AP1DelayMean'] = 0 #(2000, inplace=True)\n",
    "df['AP2DelayMean'] = 0 # .fillna(2000, inplace=True)\n",
    "\n",
    "df['AP1DelaySD'] = 0\n",
    "df['AP2DelaySD'] = 0 #.fillna(0, inplace=True)\n",
    "\n",
    "df['AP1DelayMeanStrongStim'] = 0 #.fillna(2000, inplace=True)\n",
    "df['AP2DelayMeanStrongStim'] = 0 #.fillna(2000, inplace=True)\n",
    "\n",
    "df['AP1DelaySDStrongStim'] = 0  #.fillna(0, inplace=True)\n",
    "df['AP2DelaySDStrongStim'] = 0 # .fillna(0, inplace=True)\n",
    "\n",
    "df['Burst1ISIMean'] = 0 #.fillna(2000, inplace=True)\n",
    "df['Burst1ISIMeanStrongStim'] = 0 # .fillna(2000, inplace=True)\n",
    "\n",
    "df['Burst1ISISD'] = 0 #.fillna(0, inplace=True)\n",
    "df['Burst1ISISDStrongStim'] = 0 # .fillna(0, inplace=True)\n",
    "\n",
    "df['AccommodationRateMeanAtSS'] = 0 #(2000, inplace=True)\n",
    "\n",
    "df['ISIMedian'] = 0 # .fillna(2000, inplace=True)\n",
    "\n",
    "df['ISICV'] = 0 # .fillna(0, inplace=True)\n",
    "\n",
    "df['ISIBurstMeanChange']= 0 # .fillna(0, inplace=True)\n",
    "\n",
    "df['SpikeRateStrongStim'] = 0 # .fillna(0, inplace=True)\n",
    "\n",
    "df['InputResistance'] =  0 \n",
    "#(df['InputResistance'].mean(), inplace=True)\n",
    "\n",
    "df['FrequencyPassAbove'] = 0 # .fillna(29, inplace=True)\n",
    "df['FrequencyPassBelow'] = 0 #(143, inplace=True)\n",
    "\n",
    "df['RampFirstSpike'] = 0 #(5000, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df = pd.DataFrame()\n",
    "df['AP1Amplitude'].fillna(0, inplace=True)\n",
    "df['AP2Amplitude'].fillna(0, inplace=True)\n",
    "\n",
    "df['AP1SSAmplitudeChange'].fillna(0, inplace=True)\n",
    "\n",
    "df['AP1WidthHalfHeight'].fillna(0, inplace=True)\n",
    "df['AP2WidthHalfHeight'].fillna(0, inplace=True)\n",
    "\n",
    "df['AP1WidthPeakToTrough'].fillna(0, inplace=True)\n",
    "df['AP2WidthPeakToTrough'].fillna(0, inplace=True)\n",
    "\n",
    "df['AP1RateOfChangePeakToTrough'].fillna(0, inplace=True)\n",
    "df['AP2RateOfChangePeakToTrough'].fillna(0, inplace=True)\n",
    "\n",
    "df['AP1AHPDepth'].fillna(0, inplace=True)\n",
    "df['AP2AHPDepth'].fillna(0, inplace=True)\n",
    "\n",
    "df['AP1DelayMean'].fillna(2000, inplace=True)\n",
    "df['AP2DelayMean'].fillna(2000, inplace=True)\n",
    "\n",
    "df['AP1DelaySD'].fillna(0, inplace=True)\n",
    "df['AP2DelaySD'].fillna(0, inplace=True)\n",
    "\n",
    "df['AP1DelayMeanStrongStim'].fillna(2000, inplace=True)\n",
    "df['AP2DelayMeanStrongStim'].fillna(2000, inplace=True)\n",
    "\n",
    "df['AP1DelaySDStrongStim'].fillna(0, inplace=True)\n",
    "df['AP2DelaySDStrongStim'].fillna(0, inplace=True)\n",
    "\n",
    "df['Burst1ISIMean'].fillna(2000, inplace=True)\n",
    "df['Burst1ISIMeanStrongStim'].fillna(2000, inplace=True)\n",
    "\n",
    "df['Burst1ISISD'].fillna(0, inplace=True)\n",
    "df['Burst1ISISDStrongStim'].fillna(0, inplace=True)\n",
    "\n",
    "df['AccommodationRateMeanAtSS'].fillna(2000, inplace=True)\n",
    "\n",
    "df['ISIMedian'].fillna(2000, inplace=True)\n",
    "\n",
    "df['ISICV'].fillna(0, inplace=True)\n",
    "\n",
    "df['ISIBurstMeanChange'].fillna(0, inplace=True)\n",
    "\n",
    "df['SpikeRateStrongStim'].fillna(0, inplace=True)\n",
    "\n",
    "df['InputResistance'].fillna(df['InputResistance'].mean(), inplace=True)\n",
    "\n",
    "df['FrequencyPassAbove'].fillna(29, inplace=True)\n",
    "df['FrequencyPassBelow'].fillna(143, inplace=True)\n",
    "\n",
    "df['RampFirstSpike'].fillna(5000, inplace=True)\n",
    "\n",
    "\n",
    "#a = numpy.zeros(shape=(1,len(df.columns)))\n",
    "#df = pd.Series(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for t in tests:\n",
    "    t.name = t.name.replace(\" \", \"\")\n",
    "df1 = pd.DataFrame(columns=tests)\n",
    "\n",
    "df1.loc[-1] = [ d['mean'] for d in dm_properties ]\n",
    "\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_no_trans.to_csv(\"cell-ephyz-raw.csv\")\n",
    "df.to_csv(\"cell-ephyz-transformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_column(col):\n",
    "    # Bi-symmetric log-like transformation, from: \n",
    "    # http://iopscience.iop.org/article/10.1088/0957-0233/24/2/027001/pdf\n",
    "    trans = np.sign(df[col])*np.log(1+np.abs(df[col]*2.302585))\n",
    "    df[col] = trans\n",
    "    \n",
    "df_no_trans = df.copy()\n",
    "for col in df.columns:\n",
    "    log_column(col)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "def PCA_and_Cluster(df, \n",
    "                    hide_noise = False, \n",
    "                    remove_noise = False, \n",
    "                    k_means = False, \n",
    "                    interactive=False,\n",
    "                    cluster_captions=string.ascii_uppercase,\n",
    "                    axis_captions=['PC0','PC1','PC2']):\n",
    "    \n",
    "    min_cluster_size=10 # IF k_means == False:\n",
    "    kmeans_n_clusters=6        # IF k_means == True\n",
    "\n",
    "    #Subselect rows based on selected cluster\n",
    "    df_all = df\n",
    "    #[\"ClusterPath\"].str.startswith(parent_path)]\n",
    "\n",
    "    # Perform PCA\n",
    "    ss = StandardScaler()\n",
    "    x = ss.fit_transform(df.loc[:,prop_names].values)\n",
    "    x = DataFrame(x,columns=prop_names)\n",
    "    print('start dims', len(prop_names))\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(svd_solver='full',n_components=0.95)\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "    principalDf = DataFrame(data = principalComponents)\n",
    "    X = principalDf\n",
    "    X.index = df.index\n",
    "    print('post-pca dims', len(principalDf.columns))\n",
    "    print(X.shape[0],'rows')\n",
    "\n",
    "\n",
    "    #Exploratory cluster analysis of the PCA space - 3D plot, dendrogram, and silhouette analysis\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    #%matplotlib inline\n",
    "    from mpl_toolkits import mplot3d\n",
    "    plt.figure(figsize=(15, 7))  \n",
    "    plt.axes(projection='3d')\n",
    "    plt.plot(X[0],X[1], X[2],'bo')\n",
    "    plt.show()\n",
    "\n",
    "    print(X.shape[0],'items')\n",
    "\n",
    "\n",
    "    range_n_clusters = range(2, 8)\n",
    "\n",
    "    clusters = []\n",
    "    widths = []\n",
    "    for n_clusters in range_n_clusters:\n",
    "        from sklearn.cluster import AgglomerativeClustering\n",
    "        #clusterer = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')  \n",
    "        clusterer = KMeans(n_clusters=n_clusters)  \n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "\n",
    "        clusters.append(n_clusters)\n",
    "        widths.append(silhouette_avg)\n",
    "\n",
    "    plt.plot(clusters, widths)\n",
    "    plt.show()\n",
    "\n",
    "    # Find the properties that are most highly correlated with the first 3 PCA components\n",
    "\n",
    "    %matplotlib inline\n",
    "    i=0\n",
    "    comp_names = []\n",
    "    for i in range(3):\n",
    "\n",
    "        prop_r = np.array([stats.pearsonr(X[i],df[col])[0] if stats.pearsonr(X[i],df[col])[1] < 0.001 else 0 for col in df.columns[:-3]])\n",
    "        inds = (-np.abs(prop_r)).argsort()\n",
    "    #     plt.plot(range(len(pca.components_[0])), pca.components_[i][inds])\n",
    "    #     plt.show()\n",
    "        print(np.array(prop_names)[inds][:5])\n",
    "        print(prop_r[inds][:5])\n",
    "\n",
    "        name = \"\"\n",
    "        for f in range(3):\n",
    "            name += (\"-\" if prop_r[inds][f] < 0 else \"+\") + prop_names[inds[f]]\n",
    "        comp_names.append(name)\n",
    "        print(name)\n",
    "        print(\"         -----            \")\n",
    "    \n",
    "\n",
    "    if interactive:\n",
    "        %matplotlib notebook\n",
    "        %matplotlib notebook\n",
    "\n",
    "\n",
    "    # 3D plot of clusters in PCA space\n",
    "    X_w_noise = X.copy()\n",
    "    X_w_noise[\"Cluster\"] = -1\n",
    "    X_w_noise[\"WasNoise\"] = False\n",
    "\n",
    "    if remove_noise:\n",
    "        cluster = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "        cluster.fit_predict(X)\n",
    "        X = X[cluster.labels_ != -1]\n",
    "\n",
    "    if k_means:\n",
    "        cluster = KMeans(n_clusters=kmeans_n_clusters,random_state=1)\n",
    "        cluster.fit_predict(X)\n",
    "\n",
    "    else:\n",
    "        cluster = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "        cluster.fit_predict(X)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    if hide_noise:\n",
    "        ax.scatter(\n",
    "            X[cluster.labels_ != -1][0],\n",
    "            X[cluster.labels_ != -1][1], \n",
    "            X[cluster.labels_ != -1][2], depthshade=False,marker='o', \n",
    "            c=cluster.labels_[cluster.labels_ != -1], \n",
    "            cmap='rainbow')\n",
    "\n",
    "    else:\n",
    "        ax.scatter(\n",
    "            X[0],\n",
    "            X[1], \n",
    "            X[2], depthshade=False,marker='o', \n",
    "            c=cluster.labels_, \n",
    "            cmap='rainbow')\n",
    "\n",
    "    ax.set_xlabel(axis_captions[0])\n",
    "    ax.set_ylabel(axis_captions[1])\n",
    "    ax.set_zlabel(axis_captions[2])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    centers = []\n",
    "\n",
    "    if k_means:\n",
    "        centers = cluster.cluster_centers_\n",
    "    else:\n",
    "        labels = np.unique(cluster.labels_) if not hide_noise else np.unique(cluster.labels_[cluster.labels_ != -1])\n",
    "\n",
    "        for l in labels:\n",
    "            X_label = X[cluster.labels_ == l]\n",
    "            center = [np.mean(X_label[c]) for c in range(X.shape[1])]\n",
    "            centers.append(center)\n",
    "\n",
    "    pca_centers = centers\n",
    "            \n",
    "    # Show clusters as letters in the plot\n",
    "    for i, center in enumerate(centers):\n",
    "        ax.text(center[0],center[1],center[2],cluster_captions[i],size=20)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    for key in locals().keys():\n",
    "        globals()[key] = locals()[key]\n",
    "\n",
    "    import collections\n",
    "    print(collections.Counter(cluster.labels_))\n",
    "\n",
    "    # Print cluster summary stats\n",
    "    for c, center in enumerate(centers):\n",
    "        dist = np.apply_along_axis(euclidean, 1, X, center)\n",
    "        dist_sort_is = dist.argsort()\n",
    "        from pprint import pprint as pp\n",
    "\n",
    "        pp({\"cluster\": c, \n",
    "            \"cells\": X.iloc[dist_sort_is].index[:5], \n",
    "            \"sd\":[\"{:12.2f}\".format(np.std(X.iloc[np.where(cluster.labels_ == c)][pc])) for pc in range(3)],\n",
    "            \"center\":[\"{:12.2f}\".format(c) for c in center[0:3]],\n",
    "           })\n",
    "\n",
    "    # 3D plot of clusters in RAW feature space\n",
    "    source_df = df_no_trans.ix[X.index]\n",
    "\n",
    "    display_props = [\"ISIMedian\",\"AccommodationAtSSMean\",\"AP1DelayMeanStrongStim\"]\n",
    "    #display_props = [\"AP1DelayMeanStrongStim\",\"ISIMedian\",\"AccommodationAtSSMean\"]\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    ax.scatter(\n",
    "        source_df[display_props[0]],\n",
    "        source_df[display_props[1]], \n",
    "        source_df[display_props[2]], \n",
    "        depthshade=True,\n",
    "        marker='o', \n",
    "        c=cluster.labels_, \n",
    "        cmap='rainbow')\n",
    "\n",
    "    ax.set_xlabel(display_props[0])\n",
    "    ax.set_ylabel(display_props[1])\n",
    "    ax.set_zlabel(display_props[2])\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    centers = []\n",
    "    sds = []\n",
    "\n",
    "    labels = np.unique(cluster.labels_)\n",
    "\n",
    "    print(display_props)\n",
    "\n",
    "    for i, l in enumerate(labels):\n",
    "        X_label = source_df[cluster.labels_ == l]\n",
    "        center = [np.mean(X_label[prop]) for prop in display_props]\n",
    "        centers.append(center)\n",
    "\n",
    "        sd = [np.std(X_label[prop]) for prop in display_props]\n",
    "        sds.append(sd)\n",
    "\n",
    "        ax.text(center[0],center[1],center[2],cluster_captions[i],size=20)\n",
    "\n",
    "        print(cluster_captions[i],[\"{:0.2f}+/-{:0.2f}\".format(centers[i][c],sds[i][c]) for c,_ in enumerate(center)])\n",
    "\n",
    "        reg = smf.ols('AP1DelayMeanStrongStim~ISIMedian',data=X_label).fit()\n",
    "        print('reg isi v delay params p-s r', reg._results.params, reg._results.pvalues, reg._results.rsquared_adj)\n",
    "\n",
    "        reg = smf.ols('AP1DelayMeanStrongStim~AccommodationAtSSMean',data=X_label).fit()\n",
    "        print('reg accom v delay params p-s r', reg._results.params, reg._results.pvalues, reg._results.rsquared_adj)\n",
    "        \n",
    "        print(\"delay v accom\",stats.pearsonr(X_label[\"AP1DelayMeanStrongStim\"],X_label[\"AccommodationAtSSMean\"]))\n",
    "        print(\"delay v isi\",stats.pearsonr(X_label[\"AP1DelayMeanStrongStim\"],X_label[\"ISIMedian\"]))\n",
    "\n",
    "\n",
    "    plt.show()        \n",
    "\n",
    "    #%matplotlib inline\n",
    "\n",
    "\n",
    "    # Set cluster ids in the transformed DataFrame\n",
    "    X[\"Cluster\"] = cluster.labels_\n",
    "\n",
    "    # Set cluster in the DF that also has any noise rows\n",
    "    for label in X.index:\n",
    "        X_w_noise.at[label, \"Cluster\"] = X.at[label, \"Cluster\"]    \n",
    "\n",
    "    # Assign noise models to the cluster with the closest center\n",
    "    noise_models = X_w_noise[X_w_noise[\"Cluster\"] == -1].index\n",
    "    for model in noise_models:    \n",
    "        #find the closest pca space cluster center\n",
    "        dist = np.apply_along_axis(euclidean, 1, pca_centers, X_w_noise.ix[model][:-2])\n",
    "        dist_sort_is = dist.argsort()\n",
    "        X_w_noise.at[model, \"Cluster\"] = dist_sort_is[0] #[0] stores the closest cluster ID\n",
    "        X_w_noise.at[model, \"WasNoise\"] = True\n",
    "\n",
    "    df[\"Cluster\"] = X_w_noise[\"Cluster\"]\n",
    "    df[\"WasNoise\"] = X_w_noise[\"WasNoise\"]\n",
    "    df[\"ClusterPath\"] = parent_path + df[\"Cluster\"].map(str) + \"/\"\n",
    "    \n",
    "\n",
    "    for label in df.index:\n",
    "        df_all.at[label, \"ClusterPath\"] = df.at[label, \"ClusterPath\"]\n",
    "        df_all.at[label, \"Cluster\"] = df.at[label, \"Cluster\"]\n",
    "        df_all.at[label, \"WasNoise\"] = df.at[label, \"WasNoise\"]\n",
    "\n",
    "    # Sanity checks\n",
    "    print('current subset clusters',np.unique(df[\"ClusterPath\"]))\n",
    "    print('all clusters',np.unique(df_all[\"ClusterPath\"]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PCA_and_Cluster(parent_path = \"/\", \n",
    "                hide_noise = True, \n",
    "                remove_noise = False, \n",
    "                k_means = False, \n",
    "                interactive=False,\n",
    "                cluster_captions=['AS','npRB','RA','MS'],\n",
    "                axis_captions=['PC1 ~Delay to 1st AP (StDevs)', 'PC2 ~2nd AP Amplitude Change (StDevs)', 'PC3 ~AP Width (StDevs)'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
