{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1.\n",
    "### Next Chapter\n",
    "In chapter 2 (a closely related notebook see hyperlink) \n",
    "I demonstrate optimization using spike time statistics via the allen SDK\n",
    "[Notebook 2](allen_test_example2.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get optimization results.\n",
    "\n",
    "Either preload pre-optimized data for five different experimental cell types, ir in the absence of data, do the optimization in place below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up environment.\n",
    "In the cell below we set up an environment that supports visualization of \n",
    "pre-computed optimization results. This also includes download of the results.\n",
    "\n",
    "This also includes forcing a notebook compliant plotting backend initialization, resulting in an empty graph, for the time being"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'inject_and_plot' from 'neuronunit.optimisation.optimization_management' (/home/user/safe2/neuronunit/neuronunit/optimisation/optimization_management.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-92f25d08aeee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneuronunit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimisation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization_management\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minject_and_plot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnuunit_dm_evaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'inject_and_plot' from 'neuronunit.optimisation.optimization_management' (/home/user/safe2/neuronunit/neuronunit/optimisation/optimization_management.py)"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot([0],[1])\n",
    "\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "from neuronunit.optimisation.optimization_management import inject_and_plot, nuunit_dm_evaluation, transform\n",
    "import os\n",
    "try:\n",
    "    import efel\n",
    "except:\n",
    "    !pip install efel\n",
    "from neuronunit.optimisation.get_three_feature_sets_from_nml_db import three_feature_sets_on_static_models\n",
    "\n",
    "\n",
    "try:\n",
    "    results = pickle.load(open('../unit_test/working/all_data_tests.p','rb'))\n",
    "except:\n",
    "    try:\n",
    "\n",
    "        os.system('wget https://www.dropbox.com/s/cod7jz4yrr55dsw/all_data_tests.p?dl=0')\n",
    "        results = pickle.load(open('../unit_test/working/all_data_tests.p?dl=0','rb'))\n",
    "    except:\n",
    "        # No data available, so lets generate data in place below:\n",
    "        # Do the optization in place.\n",
    "        import elephant_data_tests\n",
    "\n",
    "        results = pickle.load(open('../unit_test/working/all_data_tests.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in pre-wrangled/refined data\n",
    "that was output from a previous optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "result_RAW = pickle.load(open('RAWall_data_tests.p','rb'))\n",
    "result_ADEXP = pickle.load(open('ADEXPall_data_tests.p','rb'))\n",
    "result_RAW = result_RAW['RAW']\n",
    "\n",
    "electro_path = str(os.getcwd())+'/../tests/russell_tests.p'\n",
    "\n",
    "assert os.path.isfile(electro_path) == True\n",
    "with open(electro_path,'rb') as f:\n",
    "    (test_frame,obs_frame) = pickle.load(f)\n",
    "filtered_tests = {key:val for key,val in test_frame.items()}\n",
    "from neuronunit.optimisation.optimization_management import OptMan,TSD, get_dtc_pop\n",
    "from neuronunit.optimisation import model_parameters\n",
    "print('Neuronunit tests used to constrain models {0}'.format(filtered_tests.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(result_ADEXP['ADEXP']['olf'][0])\n",
    "ad_olf_dtc = result_ADEXP['ADEXP']['olf'][0].dtc\n",
    "ad_purkine_dtc = result_ADEXP['ADEXP']['purkine'][0].dtc\n",
    "ad_ca1pyr_dtc = result_ADEXP['ADEXP']['ca1pyr'][0].dtc\n",
    "ad_ca1basket_dtc = result_ADEXP['ADEXP']['ca1basket'][0].dtc\n",
    "ad_neo_dtc = result_ADEXP['ADEXP']['neo'][0].dtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(result_ADEXP['ADEXP']['olf'][0])\n",
    "olf_dtc = result_RAW['olf'][0]\n",
    "purkine_dtc = result_RAW['purkine'][0]\n",
    "ca1pyr_dtc = result_RAW['ca1pyr'][0]\n",
    "ca1basket_dtc = result_RAW['ca1basket'][0]\n",
    "neo_dtc = result_RAW['neo'][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Iterated Plots Below:\n",
    "For every model pertaining to a different experimental cell show the rheobase spike waveform shape for the best  solution from the two different classes of optimized models.\n",
    "\n",
    "This is an indirect way of cross checking that optimizer worked, as it exerted the same constraints on different neural models.\n",
    "\n",
    "Initially you can see that spike onset time was not a feature used to constrain models, therefore the two different model classes vary a lot in spike onset time, however, we were less interested in spike timing, and more interested in waveform shape properties. Therefore in second form of plots (scroll down), one can see that spike onset time has been artificially controlled for in the spike visualization be re-aligning waveforms.\n",
    "\n",
    "The adpative exponential model has an artificial triangular appearance, only because as model developers we realized we could further optimize the cells experimental agreement, by adding in code hacks to improve the cells spike amplitude.\n",
    "\n",
    "Without this code hack, waveform shapes look deceptively dissimilar to a human viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = inject_and_plot([olf_dtc],second_pop=[ad_olf_dtc],third_pop=[ad_olf_dtc],snippets=True,experimental_cell_type='Olfactory Mitral Cell')\n",
    "_ = inject_and_plot([ca1basket_dtc],second_pop=[ad_ca1basket_dtc],third_pop=[ad_ca1basket_dtc],snippets=True,experimental_cell_type='CA1 Basket Cell')\n",
    "_ = inject_and_plot([neo_dtc],second_pop=[ad_neo_dtc],third_pop=[ad_neo_dtc],snippets=True,experimental_cell_type='Neo Cortical Pyramidal Cell')\n",
    "_ = inject_and_plot([ca1pyr_dtc],second_pop=[ad_ca1pyr_dtc],third_pop=[ad_ca1pyr_dtc],snippets=True,experimental_cell_type='CA! Cortical Pyramidal Cell')\n",
    "_ = inject_and_plot([purkine_dtc],second_pop=[ad_purkine_dtc],third_pop=[ad_purkine_dtc],snippets=True,experimental_cell_type='Cerebellar Purkinje Cell')\n",
    "\n",
    "_ = inject_and_plot([olf_dtc],second_pop=[ad_olf_dtc],third_pop=[ad_olf_dtc],experimental_cell_type='Olfactory Mitral Cell')\n",
    "_ = inject_and_plot([ca1basket_dtc],second_pop=[ad_ca1basket_dtc],third_pop=[ad_ca1basket_dtc],experimental_cell_type='CA1 Basket Cell')\n",
    "_ = inject_and_plot([neo_dtc],second_pop=[ad_neo_dtc],third_pop=[ad_neo_dtc],experimental_cell_type='Neo Cortical Pyramidal Cell')\n",
    "_ = inject_and_plot([ca1pyr_dtc],second_pop=[ad_ca1pyr_dtc],third_pop=[ad_ca1pyr_dtc],experimental_cell_type='CA! Cortical Pyramidal Cell')\n",
    "_ = inject_and_plot([purkine_dtc],second_pop=[ad_purkine_dtc],third_pop=[ad_purkine_dtc],experimental_cell_type='Cerebellar Purkinje Cell')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW = {str(str(k).split('_RAW')[0]):v for k,v in results.items() if \"_RAW\" in k}\n",
    "ADEXP = {str(str(k).split('_ADEXP')[0]):v for k,v in results.items() if \"_ADEXP\" in k}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW.keys()\n",
    "ADEXP.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_dtc_d = {}\n",
    "ADEXP_dtc_d = {}\n",
    "RAW_dtc = []\n",
    "ADEXP_dtc = []\n",
    "for k in RAW.keys():\n",
    "\n",
    "    #RAW_dtc_d[k] = [d.dtc for d in RAW[k]]\n",
    "    \n",
    "    #ADEXP_dtc_d[k] = [d.dtc for d in ADEXP[k]]\n",
    "    \n",
    "    RAW_dtc = [d.dtc for d in RAW[k]]\n",
    "    \n",
    "    ADEXP_dtc = [d.dtc for d in ADEXP[k]]\n",
    "    print(k)\n",
    "    last_cell_type = k\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Diversity of Optimization solution sets:\n",
    "### Plot all optimized cells from the pareto front fo just one class of cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this is to show that the pareto front from the converged genetic algorithm, retains much important variation in spike waveform shape.\n",
    "\n",
    "Diversity of firing shape is retained in the NSGA3 algorithm, by deliberatley favoring collections of solutions, consisting of vary different parameter sets (solution hyper volume is maximized as one of many optimization criterion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = inject_and_plot(RAW_dtc,second_pop=ADEXP_dtc,experimental_cell_type=last_cell_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = inject_and_plot(RAW_dtc,second_pop=ADEXP_dtc,snippets=True,experimental_cell_type=last_cell_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thoroughly interrogate Neuron Unit test scores.\n",
    "by looking into observation/prediction agreement for the two different models classes, in the case of optimizing against one particular experimental cell. In priciple this can easily be done for all of the different experimental cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import copy\n",
    "RAW_dtc = {}\n",
    "ADEXP_dtc = {}\n",
    "for k in RAW.keys():\n",
    "    print(k)\n",
    "    RAW_dtc[k] = copy.copy([d.dtc for d in RAW[k]])\n",
    "    \n",
    "    ADEXP_dtc[k] = copy.copy([d.dtc for d in ADEXP[k]])\n",
    "    df1 = None\n",
    "    df0 = None\n",
    "    df1 = pd.DataFrame([RAW_dtc[k][0].scores])\n",
    "    df1= df1.rename(index={0: str('Izhikivitch')})\n",
    "    df0 = pd.DataFrame([ADEXP_dtc[k][0].scores])\n",
    "    df0 = df0.rename(index={0: str('ADEXP')})\n",
    "    df2 = df0.append(df1)\n",
    "    df3 = df2.T\n",
    "    display(df3)\n",
    "    #df3 = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in RAW.keys():\n",
    "    print(k)\n",
    "    RAW_dtc[k] = copy.copy([d.dtc for d in RAW[k]])\n",
    "    \n",
    "    ADEXP_dtc[k] = copy.copy([d.dtc for d in ADEXP[k]])\n",
    "    df1 = None\n",
    "    df0 = None\n",
    "    df1 = pd.DataFrame([RAW_dtc[k][0].scores])\n",
    "    df1= df1.rename(index={0: str('Izhikivitch')})\n",
    "    df0 = pd.DataFrame([ADEXP_dtc[k][0].scores])\n",
    "    df0 = df0.rename(index={0: str('ADEXP')})\n",
    "    df2 = df0.append(df1)\n",
    "    df3 = df2.T\n",
    "    display(df3)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above it looks like the adaptive expontial model struggles to recaptiluate an experimental value of a rheobase test current injection, but does well enough on most other tests. It even outperforms the izhikitch model on Injected Amplitude accuracy, Input resistance accuracy, Resting potenial accuracy, and the time constant test.\n",
    "\n",
    "The izhikevitch model has a different strength weekness profile. The izhikitch model is unable to recapitulate  experimental data for the input resistance test, but it does well better than the adaptive expontial model at matching the experimental capacitance, and matching the experimental width to name a few scores. The adaptive exponential model is able to do well at matching experimental rheobase values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_dtc = RAW_dtc[k]\n",
    "ADEXP_dtc = ADEXP_dtc[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare0 = {t.name:t.observation for t in RAW_dtc[0].tests }\n",
    "compare0 = {k:(v,RAW_dtc[0].predictions[k]) for k,v in compare0.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare1 = {t.name:t.observation for t in ADEXP_dtc[0].tests }\n",
    "compare1 = {k:(v,RAW_dtc[0].predictions[k]) for k,v in compare1.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df = {}\n",
    "def format_nice_frame(compare0):\n",
    "    for k,v in compare0.items():\n",
    "        temp = v[0][list(v[0].keys())[0]].rescale(v[1][list(v[1].keys())[0]].units)\n",
    "        pre_df[k] = (temp,v[1][list(v[1].keys())[0]],v[0]['std'].rescale(v[1][list(v[1].keys())[0]].units))\n",
    "        df6=pd.DataFrame([pre_df])    \n",
    "\n",
    "    return df6  \n",
    "df6 = format_nice_frame(compare0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare observations and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = format_nice_frame(compare1)#compare1\n",
    "df7.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at where the optimized cells reside in druckman feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_dtc[0].attrs.pop('Iext',None)\n",
    "attrsf0=pd.DataFrame([RAW_dtc[0].attrs])\n",
    "attrsf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_dtc[-1].attrs.pop('Iext',None)\n",
    "attrsf1=pd.DataFrame([RAW_dtc[-1].attrs])\n",
    "attrsf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In notebook cells below we compute Druckman and Allen SDK\n",
    "Features on the optimized cells, in order to see if the optimized cells fall into pre-defined clusters, differentally  associated with experimental data, and model output data.\n",
    "\n",
    "To say this another way, it has previously been observed that experimental cell data, and model data, falls into easily seperated categories in feature space. This probably reflects deficits in simple model realism.\n",
    "\n",
    "By using optimized cells as new data points, and plotting their positions in a reduced dimension feature space we will be able to see if optimized models, are more convincing imitations of experimental data, by testing if new data points are harder to seperate from the experimental data.\n",
    "### Get Druckman features using a parallel algorithm to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "bagged = db.from_sequence(RAW_dtc)\n",
    "druckman_feature_coordinates_izhi = list(bagged.map(nuunit_dm_evaluation).compute())\n",
    "izhidfeatures = [d.dm_test_features for d in druckman_feature_coordinates_izhi ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck0 = pd.DataFrame([izhidfeatures[0]])\n",
    "druck0 = druck0.rename(index={0: str('Izhikivitch')})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "bagged = db.from_sequence(ADEXP_dtc)\n",
    "druckman_feature_coordinatesadexp = list(bagged.map(nuunit_dm_evaluation).compute())\n",
    "dfeatures = [d.dm_test_features for d in druckman_feature_coordinatesadexp ]\n",
    "druck1 = pd.DataFrame([dfeatures[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Get Allen features with a serial algorithm.\n",
    "TODO: This could easily be done in parallel too, by appropriating the code idiom used in the Druckman case.\n",
    "\n",
    "using a parallel algorithm to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_dtc = [d.dtc for d in RAW[k]]\n",
    "\n",
    "from neuronunit.optimisation.optimization_management import just_allen_predictions\n",
    "RAW_dtc = [d.dtc for d in RAW[k]]\n",
    "RAW_dtc[0].protocols[1].keys()\n",
    "\n",
    "RAW_dtc[0].ampl = RAW_dtc[0].protocols[1]['injected_square_current']['amplitude']*1.5\n",
    "RAW_dtc[0].ampl\n",
    "#for i in RAW_dtc:\n",
    "    \n",
    "def cell_to_allen(dtc):    \n",
    "    dtc.pre_obs = None\n",
    "    dtc.ampl = dtc.protocols[1]['injected_square_current']['amplitude']*4.5\n",
    "    dtc = just_allen_predictions(dtc)\n",
    "    return dtc\n",
    "bagged = db.from_sequence(RAW_dtc)\n",
    "RAW_dtc = list(bagged.map(cell_to_allen).compute())\n",
    "#print(RAW_dtc\n",
    "    #i.preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [i for i in RAW_dtc[0].preds]\n",
    "preds\n",
    "RAW_dtc[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = [i for i in RAW_dtc[1][0].preds]\n",
    "allen_df = pd.DataFrame(preds)\n",
    "allen_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck1\n",
    "druck1 = druck1.rename(index={0: str('ADEXP')})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck4 = pd.DataFrame([izhidfeatures[-1]])\n",
    "druck4 = druck4.rename(index={0: str('Izhi_last')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck3 = pd.DataFrame([dfeatures[-1]])\n",
    "druck3 = druck3.rename(index={0: str('ADEXP_last')})\n",
    "#druck3.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck2 = druck1\n",
    "druck2 = druck2.append(druck0)\n",
    "druck2 = druck2.append(druck3)\n",
    "druck2 = druck2.append(druck4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results['Cerebellum Purkinje cell_RAW'] \n",
    "cwd = os.getcwd()\n",
    "import numpy as np\n",
    "# Open the 1.5x rheobase file\n",
    "filename = os.path.join(cwd,'onefive_df.pkl')\n",
    "with open(filename, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "    \n",
    "df.iloc[0][192:195]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# A function to convert all cells containing array (or other things) into floats.  \n",
    "def f(x):\n",
    "    try:\n",
    "        return np.mean(x)\n",
    "    except:\n",
    "        try:\n",
    "            return np.mean(x['pred'])\n",
    "        except:\n",
    "            print(x)\n",
    "            raise e\n",
    "druck2 = druck2.fillna(0).applymap(f)\n",
    "#d2 = druck2.fillna(0).applymap(f)\n",
    "\n",
    "\n",
    "#druck2.T\n",
    "druck2\n",
    "\n",
    "for col in druck2.columns:\n",
    "    for dm in df.columns:\n",
    "        if col in dm:\n",
    "            #print(col,dm)\n",
    "            #col = dm\n",
    "            druck2.rename(columns={col:dm},inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Random Forests, variance Explained.\n",
    "## Then Throw Away Feature Dimensions\n",
    "Throw that maximally seperate experimental cells, and models. This is done by using random forests to find features that explain the most variance.\n",
    "\n",
    "The reason for this, is for each dimension we through away helps us deduce if that variance explained by that feature difference was crucial for seperating experimental data from models. In other words, we can identify weaker aspects  of models, and better target improvement areas of model performance.\n",
    "\n",
    "Previous work has suggested models fail to mimic experimental cells in all the mannerisms pertaininig to features deleted in this data frame below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# in order to find out what is seperating and what is not.\n",
    "\n",
    "try:\n",
    "    del df['InputResistanceTest_1.5x']\n",
    "    del df['InputResistanceTest_3.0x']\n",
    "\n",
    "    del df['ohmic_input_resistance_1.5x']\n",
    "    del df['ohmic_input_resistance_3.0x']\n",
    "    del df['time_1.5x']                              \n",
    "    #       0.190362\n",
    "    del df['decay_time_constant_after_stim_3.0x']\n",
    "    del df['voltage_deflection_3.0x']\n",
    "    del df['steady_state_hyper_3.0x']\n",
    "    del df['steady_state_voltage_stimend_3.0x']\n",
    "    del df['voltage_deflection_vb_ssse_3.0x']\n",
    "    del df['sag_amplitude_3.0x']\n",
    "    #0.198310\n",
    "    del df['is_not_stuck_1.5x']\n",
    "except:\n",
    "    print('features allready deleted.')\n",
    "\n",
    "\n",
    "# Apply this function to each dataframe in order to convert all cells into floats.\n",
    "# Also call fillna() first to impute missing values with 0's.  \n",
    "df = df.fillna(0).applymap(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(druck2)\n",
    "df.columns\n",
    "df\n",
    "list_cols=[]\n",
    "for col in df.columns:\n",
    "    for j in druck2.columns:\n",
    "        if j in col:\n",
    "            for i in range(0,len(druck2)):\n",
    "\n",
    "                #df.append(pd.Series(), ignore_index=True)\n",
    "\n",
    "                df.ix[len(df)-1, col] = druck2.ix[i,j]\n",
    "                #print(df.ix[-1, col])\n",
    "                #print(col)\n",
    "                list_cols.append(col)\n",
    "for col in list_cols:                \n",
    "    print(df.ix[-1,col])            \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck2\n",
    "#print(\"There are %d models+data and %d features\" % df.shape)\n",
    "# Special stuff to import\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "druck2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all features into Normal(0,1) variables\n",
    "# Important since features all have different scales\n",
    "from sklearn.preprocessing import StandardScaler#, PCA\n",
    "ss = StandardScaler()\n",
    "druck2[:] = ss.fit_transform(druck2.values)\n",
    "druck2.groupby(druck2.index).first()\n",
    "druck2 = pd.DataFrame.drop_duplicates(druck2)\n",
    "print(len(druck2))\n",
    "df = druck2\n",
    "\n",
    "#df.fillna(0).applymap(f)\n",
    "\n",
    "#new_models_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_models_idx = list(range(1666,1671,1))\n",
    "new_models_df = df[df.index.isin(new_models_idx)]\n",
    "len(new_models_df)\n",
    "new_models_idx\n",
    "new_models_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see what is there.  Might also check to see if there is data there.\n",
    "#df_30x#.head()\n",
    "\n",
    "# make model dataframe\n",
    "model_idx = [idx for idx in df.index.values if type(idx)==str]\n",
    "model_no_trans_df = df[df.index.isin(model_idx)]\n",
    "model_no_trans_df.index.name = 'Cell_ID'\n",
    "model_df = model_no_trans_df.copy()\n",
    "model_df.index.name = 'Cell_ID'\n",
    "\n",
    "# make experiment dataframe\n",
    "experiment_idx = [idx for idx in df.index.values if type(idx)==int]\n",
    "experiment_no_trans_df = df[df.index.isin(experiment_idx)]\n",
    "experiment_df = experiment_no_trans_df.copy()\n",
    "print(len(experiment_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(experiment_df))\n",
    "experiment_df = pd.DataFrame.drop_duplicates(experiment_df)\n",
    "#experiment_df.index\n",
    "experiment_df.groupby(experiment_df.index).first()\n",
    "len(experiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_df))\n",
    "model_df = pd.DataFrame.drop_duplicates(model_df)\n",
    "model_df.index\n",
    "model_df.groupby(model_df.index).first()\n",
    "\n",
    "len(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Do PCA and look at variance explained\n",
    "pca = PCA()\n",
    "pca.fit(df.T.values)\n",
    "n_features = df.shape[1]\n",
    "plt.plot(range(1,n_features+1),pca.explained_variance_ratio_.cumsum())\n",
    "plt.xlim(0,50);\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('Fraction of variance explained');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THe transformed values, ordered from highest to lowest variance dimensions\n",
    "transformed = pca.transform(df.values)\n",
    "\n",
    "\n",
    "#model_idx = [idx for idx in enumerate(df.index.values) if type(idx)==str]\n",
    "'''\n",
    "#label_model_no_trans_df\n",
    "model_no_trans_df.index.name = 'Cell_ID'\n",
    "model_df = model_no_trans_df.copy()\n",
    "model_df.index.name = 'Cell_ID'\n",
    "'''\n",
    "# make experiment dataframe\n",
    "experiment_idx = [idx for idx in df.index.values if type(idx)==int]\n",
    "model_no_trans_df = df[~df.index.isin(experiment_idx)]\n",
    "experiment_idx_labels = [(i,idx) for i,idx in enumerate(df.index.values) if type(idx)==int]\n",
    "\n",
    "#model_df\n",
    "#df.labels\n",
    "model_no_trans_df\n",
    "experiment_idx_labels = [i[0] for i in experiment_idx_labels]\n",
    "experiment_idx_labels\n",
    "model_no_trans_df\n",
    "model_index_labels = ~df.index.isin(experiment_idx)\n",
    "\n",
    "model_index_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do an isomap embedding in 2 dimensions\n",
    "isomap = Isomap(n_components=2)\n",
    "isomap.fit(df.values)\n",
    "iso = isomap.embedding_.T\n",
    "# Plot that isomap embedding. Each is a model (or a cell, for data)\n",
    "#plt.scatter(iso);\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.scatter(iso[0,experiment_idx_labels],iso[1,experiment_idx_labels],c='blue',cmap='rainbow',label='data')\n",
    "plt.scatter(iso[0,model_index_labels],iso[1,model_index_labels],c='red',cmap='rainbow',label='models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = 1.5\n",
    "strong = 3.0\n",
    "easy_map = [\n",
    "            {'AP12AmplitudeDropTest':standard},\n",
    "            {'AP1SSAmplitudeChangeTest':standard},\n",
    "            {'AP1AmplitudeTest':standard},\n",
    "            {'AP1WidthHalfHeightTest':standard},\n",
    "            {'AP1WidthPeakToTroughTest':standard},\n",
    "            {'AP1RateOfChangePeakToTroughTest':standard},\n",
    "            {'AP1AHPDepthTest':standard},\n",
    "            {'AP2AmplitudeTest':standard},\n",
    "            {'AP2WidthHalfHeightTest':standard},\n",
    "            {'AP2WidthPeakToTroughTest':standard},\n",
    "            {'AP2RateOfChangePeakToTroughTest':standard},\n",
    "            {'AP2AHPDepthTest':standard},\n",
    "            {'AP12AmplitudeChangePercentTest':standard},\n",
    "            {'AP12HalfWidthChangePercentTest':standard},\n",
    "            {'AP12RateOfChangePeakToTroughPercentChangeTest':standard},\n",
    "            {'AP12AHPDepthPercentChangeTest':standard},\n",
    "            {'InputResistanceTest':str('ir_currents')},\n",
    "            {'AP1DelayMeanTest':standard},\n",
    "            {'AP1DelaySDTest':standard},\n",
    "            {'AP2DelayMeanTest':standard},\n",
    "            {'AP2DelaySDTest':standard},\n",
    "            {'Burst1ISIMeanTest':standard},\n",
    "            {'Burst1ISISDTest':standard},\n",
    "            {'InitialAccommodationMeanTest':standard},\n",
    "            {'SSAccommodationMeanTest':standard},\n",
    "            {'AccommodationRateToSSTest':standard},\n",
    "            {'AccommodationAtSSMeanTest':standard},\n",
    "            {'AccommodationRateMeanAtSSTest':standard},\n",
    "            {'ISICVTest':standard},\n",
    "            {'ISIMedianTest':standard},\n",
    "            {'ISIBurstMeanChangeTest':standard},\n",
    "            {'SpikeRateStrongStimTest':strong},\n",
    "            {'AP1DelayMeanStrongStimTest':strong},\n",
    "            {'AP1DelaySDStrongStimTest':strong},\n",
    "            {'AP2DelayMeanStrongStimTest':strong},\n",
    "            {'AP2DelaySDStrongStimTest':strong},\n",
    "            {'Burst1ISIMeanStrongStimTest':strong},\n",
    "            {'Burst1ISISDStrongStimTest':strong},\n",
    "        ]\n",
    "\n",
    "dm_labels = [list(keys.keys())[0]+str('_')+str(list(keys.values())[0])+str('x') for keys in easy_map ]\n",
    "#dm_labels['AHP_depth_abs_slow_1.5x']\n",
    "\n",
    "#dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "druck2.columns        \n",
    "#df.append(druck2)\n",
    "druck2\n",
    "#df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck2.columns\n",
    "df.append(druck2)\n",
    "df['AP12AmplitudeDropTest_1.5x']\n",
    "druck2['AP12AmplitudeDropTest_1.5x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df['AP12AmplitudeDropTest_1.5x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
