{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "# Chapter 1.\nIn this chapter we will look at how the sciunit optimizer is readily used to fit abstract neuronal model behavior to experimentally recorded neuron waveform shapes. You can think of this problem as a type of inverse search, where we have experimental data that can be used to constrain a waveform shape. There are lots of different possible parameterizations of models we will explore, and we want to select the particular model parameterization that best agrees with experimentally recorded measurements. \n\nGolowasch, J., Goldman, M., Abbott, L.F, and Marder, E. (2002)\nFailure of averaging in the construction\nof conductance-based neuron models. J. Neurophysiol., 87: 11291131.\n\n### Next Chapters\nIn **chapter 2**  We demonstrate optimization using spike time statistics via the allen SDK\n[Chapter 2](chapter2.ipynb)\n\nIn **chapter 3**  We will take a closer at the data used to perform the fits in this notebook.\n[Chapter 3](chapter3.ipynb)\n\nIn **chapter 5** We will look at projections of Optimized cells onto a Druckman feature space, we will also look at extracting Allen SDK features from the optimized cells.\n[Chapter 3](chapter5.ipynb)\n\nIn **chapter 8** We look at building ***neuronunit*** modal data tests from neuroelectro data.\n[Chapter 9](chapter8.ipynb)\n\nIn **chapter 9** We look at class relationships, optimizing syntax, and an exhaustive search approach to ground truths.\n[Chapter 9](chapter9.ipynb)\n\nThe table below includes the five different varieties of experimental cell data that we will demonstrate how to optimize against. We will find solutions to 10 different problems.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "\n| Cell Type   |      Model Type1       |     Model Type2     | simulator backend |\n|----------|----------|----------|:-------------:|\n| Cerebellum Purkinje cell   | Adaptive Exponential | Izhikitich Model   |brian2,Forward Euler|\n| Olfactory bulb (main) mitral cell |    Adaptive Exponential | Izhikitich Model   |brian2,Forward Euler|\n| Hippocampus CA1 pyramidal cell\t | Adaptive Exponential | Izhikitich Model |brian2,Forward Euler|\n| Neocortex pyramidal cell layer 5-6 | Adaptive Exponential | Izhikitich Model|brian2,Forward Euler|\n| Hippocampus CA1 basket cell | Adaptive Exponential | Izhikitich Model|brian2,Forward Euler|\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "### Get optimization results.\nIn the cells below we either preload pre-optimized data for five different experimental cell types, ir in the absence of data, do the optimization in place below."
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "Set up environment.\nIn the cell below we set up an environment that supports visualization of \npre-computed optimization results. This also includes download of the results. This also includes forcing a notebook compliant plotting backend initialization."
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'neuromldb'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8dce1cdf98ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneuronunit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimisation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_three_feature_sets_from_nml_db\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthree_feature_sets_on_static_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../unit_test/working/all_data_tests.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/git/safe/neuronunit/neuronunit/optimisation/get_three_feature_sets_from_nml_db.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_default_https_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_unverified_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mneuromldb\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeuroMLDBStaticModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mneuronunit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimisation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdm_test_interoperable\u001b[0m \u001b[0;31m#import Interoperabe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neuromldb'"
          ]
        }
      ],
      "source": "%%capture\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport sys\nimport pickle\nimport os\n\nfrom neuronunit.optimisation.optimization_management import TSD, get_dtc_pop\nfrom neuronunit.optimisation import model_parameters\n\n\nimport nbimporter\n\nfrom importlib import reload\nimport chapter5 as chapter5\nimport pandas as pd\n\nimport efel\nimport tabulate\n\n\nfrom neuronunit.optimisation.get_three_feature_sets_from_nml_db import three_feature_sets_on_static_models\ntry:\n    results = pickle.load(open('../unit_test/working/all_data_tests.p','rb'))\nexcept:\n    try:\n        os.system('wget https://www.dropbox.com/s/cod7jz4yrr55dsw/all_data_tests.p?dl=0')\n        results = pickle.load(open('../unit_test/working/all_data_tests.p?dl=0','rb'))\n    except:\n        import elephant_data_tests\n\n        # No data available, so lets generate data in place below:\n        # Do the optization in place.\n        results = pickle.load(open('../unit_test/working/all_data_tests.p','rb'))"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "### Load in pre-wrangled/refined data\nthat was output from a previous optimization process."
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Neuronunit tests used to constrain models against these experimental cell types dict_keys(['Cerebellum Purkinje cell', 'Olfactory bulb (main) mitral cell', 'Hippocampus CA1 pyramidal cell', 'Neocortex pyramidal cell layer 5-6', 'Hippocampus CA1 basket cell'])\n"
        }
      ],
      "source": [
        "import os\n",
        "result_RAW = pickle.load(open('RAWall_data_tests.p','rb'))\n",
        "result_ADEXP = pickle.load(open('ADEXPall_data_tests.p','rb'))\n",
        "result_RAW = result_RAW['RAW']\n",
        "\n",
        "electro_path = str(os.getcwd())+'/../tests/russell_tests.p'\n",
        "\n",
        "assert os.path.isfile(electro_path) == True\n",
        "with open(electro_path,'rb') as f:\n",
        "    (test_frame,obs_frame) = pickle.load(f)\n",
        "filtered_tests = {key:val for key,val in test_frame.items()}\n",
        "\n",
        "print('Neuronunit tests used to constrain models against these experimental cell types {0}'.format(filtered_tests.keys()))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
      },
      "outputs": [
      ],
      "source": [
        "\n",
        "\n",
        "ad_olf_dtc = result_ADEXP['ADEXP']['olf'][0].dtc\n",
        "ad_purkine_dtc = result_ADEXP['ADEXP']['purkine'][0].dtc\n",
        "ad_ca1pyr_dtc = result_ADEXP['ADEXP']['ca1pyr'][0].dtc\n",
        "ad_ca1basket_dtc = result_ADEXP['ADEXP']['ca1basket'][0].dtc\n",
        "ad_neo_dtc = result_ADEXP['ADEXP']['neo'][0].dtc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
      },
      "outputs": [
      ],
      "source": [
        "olf_dtc = result_RAW['olf'][0]\n",
        "purkine_dtc = result_RAW['purkine'][0]\n",
        "ca1pyr_dtc = result_RAW['ca1pyr'][0]\n",
        "ca1basket_dtc = result_RAW['ca1basket'][0]\n",
        "neo_dtc = result_RAW['neo'][0]\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
      },
      "outputs": [
      ],
      "source": [
        "#dir(neo_dtc)\n",
        "neo_dtc.scores_ratio\n",
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    import nbimporter\n",
        "except:\n",
        "    !pip install nbimporter\n",
        "    import nbimporter\n",
        "from importlib import reload\n",
        "import chapter5 as chapter5\n",
        "\n",
        "#reload(chapter5)\n",
        "to_df = []\n",
        "ADEXP_dtc,RAW_dtc = chapter5.result_to_dict(result_ADEXP,result_RAW)\n",
        "for k in ADEXP_dtc.keys():\n",
        "    scores_ratio = {}\n",
        "    scores_ratio['forward_euler_izhikitich'] = RAW_dtc[k][0].scores_ratio\n",
        "    scores_ratio['brian2_adaptive_exponential'] = ADEXP_dtc[k][0].scores_ratio\n",
        "    to_df.append(scores_ratio)\n",
        "dfsr = pd.DataFrame(to_df)    \n",
        "for i,k in enumerate(ADEXP_dtc.keys()):\n",
        "    dfsr = dfsr.rename(index={i:k})\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "In the data frame below you can see that the Hippocampus CA1 basket cell was the most able to impose it's constraints onto abstract models. Inhibitory neurons are often fast spiking. The mitral olfactory bulb data was also highly amenable to optimization in the case of the apative exponential model, however we should not be too surprised as the corresponding ***Neuronunit*** test suite consisted of fewer constraints."
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "The data frame below follows this pattern:\n\n| Cell Type   |       Adaptive Exponential      |     Izhikitich Model    |\n|----------|----------|:-------------:|\n| Cerebellum Purkinje cell   | ratio score 0-1, over 8 tests, lower is better |  ratio score 0-1, over 8 tests, lower is better |\n| Olfactory bulb (main) mitral cell | ratio score 0-1, over 8 tests, lower is better | ratio score 0-1, over 8 tests, lower is better   |\n| Hippocampus CA1 pyramidal cell\t | ratio score 0-1, over 8 tests, lower is better | ratio score 0-1, over 8 tests, lower is better|\n| Neocortex pyramidal cell layer 5-6 | ratio score 0-1, over 8 tests, lower is better | ratio score 0-1, over 8 tests, lower is better |\n| Hippocampus CA1 basket cell | ratio score 0-1, over 8 tests, lower is better | ratio score 0-1, over 8 tests, lower is better|\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
      },
      "outputs": [
        {
          "data": {
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>forward_euler_izhikitich</th>\n      <th>brian2_adaptive_exponential</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Cerebellum Purkinje cell</th>\n      <td>0.424728</td>\n      <td>0.665494</td>\n    </tr>\n    <tr>\n      <th>Olfactory bulb (main) mitral cell</th>\n      <td>0.230287</td>\n      <td>0.740788</td>\n    </tr>\n    <tr>\n      <th>Hippocampus CA1 pyramidal cell</th>\n      <td>0.159026</td>\n      <td>0.468669</td>\n    </tr>\n    <tr>\n      <th>Neocortex pyramidal cell layer 5-6</th>\n      <td>0.154778</td>\n      <td>0.341935</td>\n    </tr>\n    <tr>\n      <th>Hippocampus CA1 basket cell</th>\n      <td>0.155093</td>\n      <td>0.558945</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
            "text/plain": "                                    forward_euler_izhikitich  \\\nCerebellum Purkinje cell                            0.424728   \nOlfactory bulb (main) mitral cell                   0.230287   \nHippocampus CA1 pyramidal cell                      0.159026   \nNeocortex pyramidal cell layer 5-6                  0.154778   \nHippocampus CA1 basket cell                         0.155093   \n\n                                    brian2_adaptive_exponential  \nCerebellum Purkinje cell                               0.665494  \nOlfactory bulb (main) mitral cell                      0.740788  \nHippocampus CA1 pyramidal cell                         0.468669  \nNeocortex pyramidal cell layer 5-6                     0.341935  \nHippocampus CA1 basket cell                            0.558945  "
          },
          "execution_count": 6,
          "metadata": {
          },
          "output_type": "execute_result"
        }
      ],
      "source": "dfsr    "
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "We picture the CA1 Basket Cell spike waveform shape below:"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'mint_generic_model'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4d55c1b0f924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mneuronunit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimisation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization_management\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minject_and_plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minject_and_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mca1basket_dtc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msecond_pop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mad_ca1basket_dtc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthird_pop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mad_ca1basket_dtc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msnippets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexperimental_cell_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CA1 Basket Cell'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minject_and_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mca1basket_dtc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msecond_pop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mad_ca1basket_dtc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthird_pop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mad_ca1basket_dtc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msnippets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexperimental_cell_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CA1 Basket Cell'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/git/safe/neuronunit/neuronunit/plottools.py\u001b[0m in \u001b[0;36minject_and_plot\u001b[0;34m(dtc, second_pop, third_pop, figname, snippets, experimental_cell_type, ground_truth, BPO)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minject_and_plot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msecond_pop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mthird_pop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'problem'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msnippets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexperimental_cell_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"neo_cortical\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mground_truth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBPO\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"darkgrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mneuronunit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimisation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization_management\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmint_generic_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'mint_generic_model'"
          ]
        }
      ],
      "source": "from neuronunit.optimisation.optimization_management import inject_and_plot\n\n_ = inject_and_plot([ca1basket_dtc],second_pop=[ad_ca1basket_dtc],third_pop=[ad_ca1basket_dtc],snippets=True,experimental_cell_type='CA1 Basket Cell')\n_ = inject_and_plot([ca1basket_dtc],second_pop=[ad_ca1basket_dtc],third_pop=[ad_ca1basket_dtc],snippets=False,experimental_cell_type='CA1 Basket Cell')\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "from neuronunit.optimisation.optimization_management import transform, save_models_for_justas, write_opt_to_nml\ntry:\n    write_opt_to_nml(olf_dtc)\n    save_models_for_justas(olf_dtc)\nexcept:\n    pass"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "## Explanation of Iterated Plots Below:\nFor every model pertaining to a different experimental cell show the rheobase spike waveform shape for the best  solution from the two different classes of optimized models. This is an indirect way of cross checking that optimizer worked, as it exerted the same constraints on different neural models, therefore one would expect to see similar waveforms in the same plot, just with different colors.\n\nInitially you can see that spike onset time was not a feature used to constrain models, therefore the two different model classes vary a lot in spike onset time, however, we were less interested in spike timing, and more interested in waveform shape properties. Therefore in second form of plots (scroll down), one can see that spike onset time has been artificially controlled for in the spike visualization be re-aligning waveforms.\n\nThe adpative exponential model has an artificial triangular appearance, only because as model developers we realized we could further optimize the cells experimental agreement, by adding in code hacks to improve the cells spike amplitude.\n\nWithout this code modification, waveform shapes look deceptively dissimilar to a human viewer."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "\n_ = inject_and_plot([olf_dtc],second_pop=[ad_olf_dtc],third_pop=[ad_olf_dtc],snippets=True,experimental_cell_type='Olfactory Mitral Cell')\n_ = inject_and_plot([ca1basket_dtc],second_pop=[ad_ca1basket_dtc],third_pop=[ad_ca1basket_dtc],snippets=True,experimental_cell_type='CA1 Basket Cell')\n_ = inject_and_plot([neo_dtc],second_pop=[ad_neo_dtc],third_pop=[ad_neo_dtc],snippets=True,experimental_cell_type='Neo Cortical Pyramidal Cell')\n_ = inject_and_plot([ca1pyr_dtc],second_pop=[ad_ca1pyr_dtc],third_pop=[ad_ca1pyr_dtc],snippets=True,experimental_cell_type='CA1 Cortical Pyramidal Cell')\n_ = inject_and_plot([purkine_dtc],second_pop=[ad_purkine_dtc],third_pop=[ad_purkine_dtc],snippets=True,experimental_cell_type='Cerebellar Purkinje Cell')\n\n_ = inject_and_plot([olf_dtc],second_pop=[ad_olf_dtc],third_pop=[ad_olf_dtc],experimental_cell_type='Olfactory Mitral Cell')\n_ = inject_and_plot([ca1basket_dtc],second_pop=[ad_ca1basket_dtc],third_pop=[ad_ca1basket_dtc],experimental_cell_type='CA1 Basket Cell')\n_ = inject_and_plot([neo_dtc],second_pop=[ad_neo_dtc],third_pop=[ad_neo_dtc],experimental_cell_type='Neo Cortical Pyramidal Cell')\n_ = inject_and_plot([ca1pyr_dtc],second_pop=[ad_ca1pyr_dtc],third_pop=[ad_ca1pyr_dtc],experimental_cell_type='CA! Cortical Pyramidal Cell')\n_ = inject_and_plot([purkine_dtc],second_pop=[ad_purkine_dtc],third_pop=[ad_purkine_dtc],experimental_cell_type='Cerebellar Purkinje Cell')\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "#RAW.keys()\n#ADEXP.keys()\nimport pandas as pd\ndf = pd.DataFrame([ad_purkine_dtc.attrs])\ndf.T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": ""
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": ""
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "## Demonstrate Diversity of Optimization solution sets:\n### Plot all optimized cells from the pareto front fo just one class of cell."
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "The purpose of this is to show that the pareto front from the converged genetic algorithm, retains much important variation in spike waveform shape.\n\nDiversity of firing shape is retained in the NSGA3 algorithm, by deliberatley favoring collections of solutions, consisting of vary different parameter sets (solution hyper volume is maximized as one of many optimization criterion)."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "\n\nfor k,v in ADEXP_dtc.items():\n    vm = inject_and_plot(RAW_dtc[k],second_pop=ADEXP_dtc[k],experimental_cell_type=k)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "for k,v in ADEXP_dtc.items():\n\n    vm = inject_and_plot(RAW_dtc[k],second_pop=ADEXP_dtc[k],snippets=True,experimental_cell_type=k)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "#  Do a cursory check of Neuron Unit test scores.\nby looking into observation/prediction agreement for the two different models classes, in the case of optimizing against one particular experimental cell. In priciple this can easily be done for all of the different experimental cells.\n\nNote many of the tables below, are unintentional clones of each other. This is an intermittent problem to do with indexing data properly that I am currently fixing. By examining the graphs above its easy to confirm that optimization outputs appropriately vary to match specific cells.\n\nWe will more thoroughly interrogate test scores in chapter3, as this work is continued in detail in [chapter3](chapter3.ipynb)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "\nprint(result_ADEXP.keys())\nimport pandas as pd\n\nfrom IPython.display import display\nimport copy\nRAW_dtc = {}\nADEXP_dtc = {}\n#type(result_ADEXP['ADEXP']['olf'][0])\nRAW_dtc = {}\nRAW_dtc['Cerebellum Purkinje cell'] = result_RAW['purkine']\nRAW_dtc['Olfactory bulb (main) mitral cell'] = result_RAW['olf']\nRAW_dtc['Hippocampus CA1 pyramidal cell'] = result_RAW['ca1pyr']\nRAW_dtc['Neocortex pyramidal cell layer 5-6'] = result_RAW['neo']\nRAW_dtc['Hippocampus CA1 basket cell'] = result_RAW['ca1basket']\n\nADEXP_dtc = {}\nADEXP_dtc['Cerebellum Purkinje cell'] = [d.dtc for d in result_ADEXP['ADEXP']['purkine']]\nADEXP_dtc['Olfactory bulb (main) mitral cell'] = [d.dtc for d in result_ADEXP['ADEXP']['olf']]\nADEXP_dtc['Hippocampus CA1 pyramidal cell'] = [d.dtc for d in result_ADEXP['ADEXP']['ca1pyr']]\nADEXP_dtc['Neocortex pyramidal cell layer 5-6'] = [d.dtc for d in result_ADEXP['ADEXP']['neo']]\nADEXP_dtc['Hippocampus CA1 basket cell'] = [d.dtc for d in result_ADEXP['ADEXP']['ca1basket']]\n\nfor k in RAW_dtc.keys():\n    print(k)\n    df1 = None\n    df0 = None\n    df1 = pd.DataFrame([RAW_dtc[k][0].scores])\n    df1= df1.rename(index={0: str('Izhikivitch')})\n    df0 = pd.DataFrame([ADEXP_dtc[k][0].scores])\n    df0 = df0.rename(index={0: str('ADEXP')})\n    df2 = df0.append(df1)\n    df3 = df2.T\n    display(df3)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "\nmd = tabulate.tabulate(df3, headers='keys', tablefmt='pipe')\n# Fix the markdown string; it will not render with an empty first table cell,\n# so if the dataframe's index has no name, just place an 'x' there.\nmd = md.replace('|    |','| %s |' % (df.index.name if df.index.name else 'x'))\n\nfrom IPython.display import display, Markdown, Latex\ndisplay(Markdown(md))"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "In the cell above it looks like the adaptive expontial model struggles to recaptiluate an experimental value of a rheobase test current injection, but does well on most other tests. It even outperforms the izhikitch model on Injected Amplitude accuracy, Input resistance accuracy, Resting potenial accuracy, and the time constant test.\n\nThe izhikevitch model has a different strength weekness profile to the Adaptive Exponential model. The izhikitch model is unable to recapitulate  experimental data for the input resistance test, but it does well better than the adaptive expontial model at matching the experimental capacitance, and matching the experimental width to name a few scores. The adaptive exponential model is able to do well at matching experimental rheobase values."
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "# In chapter 3\nWe will go beyond Neuronunits optimization test scores and instead check for agreement between the experimental literature and model outputs.\n\n\nIn **chapter 3** (a closely related notebook see [**chapter 3**](chapter3.ipynb) \n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "The user of the Sciunit optimization can use the outuput of previous optimizations to initiate future optimization work. Optimization often involves an exploration/exploitation trade-off. The user of this optimization tool, can opt-into greater exploration of the solution space. See below:\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "```python\n%%capture\nfrom neuronunit.optimisation.optimisations import run_ga\nfrom neuronunit.optimisation import model_parameters\nMODEL_PARAMS=model_parameters.MODEL_PARAMS['ADEXP']\ntest = TSD(filtered_tests['Hippocampus CA1 pyramidal cell'])\ntest.use_rheobase_score = True\nout = run_ga(MODEL_PARAMS, 1, test, \\\n        free_params = MODEL_PARAMS.keys(), hc = None, MU = len(result_ADEXP['ADEXP']['ca1pyr']), seed_pop = result_ADEXP['ADEXP']['ca1pyr'], \\\n           backend = str('ADEXP'),protocol={'use_rheobase_score':True,'allen':False,'elephant':True})\n\nfiltered_tests.keys()\nout\n```           "
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "# End of chapter 1\n*** see chapters 2,3,4,5 ***"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "Disregard the following content which is a draft copy of chapter 5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "compare0 = {t.name:t.observation for t in RAW_dtc['Neocortex pyramidal cell layer 5-6'][0].tests.values() }\ncompare0 = {k:(v,RAW_dtc['Neocortex pyramidal cell layer 5-6'][0].predictions[k]) for k,v in compare0.items()  }"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "print([t for t in ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].tests ])"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "compare1 = {t.name:t.observation for t in ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].tests.values() }\nprint(compare1.keys())\nprint(ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].predictions.keys())\ncompare1 = {k:(v,ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].predictions[k]) for k,v in compare1.items() if k in ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].predictions.keys()}"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "\ndef format_nice_frame(compare0):\n    pre_df = {}\n    for k,v in compare0.items():\n        temp = v[0][list(v[0].keys())[0]].rescale(v[1][list(v[1].keys())[0]].units)\n        pre_df[k] = (temp,v[1][list(v[1].keys())[0]],v[0]['std'].rescale(v[1][list(v[1].keys())[0]].units))\n        df6=pd.DataFrame([pre_df])    \n\n    return df6  \ndf6 = format_nice_frame(compare0)"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "# compare observations and predictions:"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": ""
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "df6.T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "# Look at where the optimized cells reside in regular parameter space\nThe NSGA3 algorithm returns a pareto front as a solution, this front as acts as a set of diverse model parameterizations\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "RAW_dtc['Neocortex pyramidal cell layer 5-6'][0].attrs.pop('Iext',None)\nattrsf0=pd.DataFrame([RAW_dtc['Neocortex pyramidal cell layer 5-6'][0].attrs])\nattrsf0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "RAW_dtc['Neocortex pyramidal cell layer 5-6'][-1].attrs.pop('Iext',None)\nattrsf1=pd.DataFrame([RAW_dtc['Neocortex pyramidal cell layer 5-6'][-1].attrs])\nattrsf1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].attrs.pop('Iext',None)\nattrsf3=pd.DataFrame([ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].attrs])\nattrsf3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].attrs.pop('Iext',None)\nattrsf4=pd.DataFrame([ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][-1].attrs])\nattrsf4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "\n## In the notebook cells below we compute Druckman and Allen SDK\nFeatures on the optimized cells, in order to see if the optimized cells fall into pre-defined clusters, differentally  associated with experimental data, and model output data.\n\nTo say this another way, it has previously been observed that experimental cell data, and model data, falls into easily seperated categories in feature space. This probably reflects deficits in simple model realism.\n\nBy using optimized cells as new data points, and plotting their positions in a reduced dimension feature space we will be able to see if optimized models, are more convincing imitations of experimental data, by testing if new data points are harder to seperate from the experimental data.\n### Get Druckman features using a parallel algorithm to save time\nThe Dask Bag data type allows you to map embarrassingly parallel functions"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "import dask.bag as db\nfrom neuronunit.optimisation.optimization_management import nuunit_dm_evaluation\nbagged = db.from_sequence(RAW_dtc['Neocortex pyramidal cell layer 5-6'])\ndruckman_feature_coordinates_izhi = list(bagged.map(nuunit_dm_evaluation).compute())\nizhidfeatures = [d.dm_test_features for d in druckman_feature_coordinates_izhi ]\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "druck0 = pd.DataFrame([izhidfeatures[0]])\ndruck0 = druck0.rename(index={0: str('Izhikivitch')})\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "import dask.bag as db\nbagged = db.from_sequence(ADEXP_dtc['Neocortex pyramidal cell layer 5-6'])\ndruckman_feature_coordinatesadexp = list(bagged.map(nuunit_dm_evaluation).compute())\ndfeatures = [d.dm_test_features for d in druckman_feature_coordinatesadexp ]\ndruck1 = pd.DataFrame([dfeatures[0]])"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "### Get Allen features with a parallel algorithm.\nThis is done by appropriating the code idiom above used in the Druckman case.\n\nusing a parallel algorithm to save time"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "#RAW_dtc = [d.dtc for d in RAW[k]]\n\nfrom neuronunit.optimisation.optimization_management import just_allen_predictions\n#RAW_dtc = [d.dtc for d in RAW[k]]\nRAW_dtc['Neocortex pyramidal cell layer 5-6'][0].protocols[1].keys()\n\nRAW_dtc['Neocortex pyramidal cell layer 5-6'][0].ampl = RAW_dtc['Neocortex pyramidal cell layer 5-6'][0].protocols[1]['injected_square_current']['amplitude']*1.5\nRAW_dtc['Neocortex pyramidal cell layer 5-6'][0].ampl\n#for i in RAW_dtc:\n    \ndef cell_to_allen(dtc):    \n    dtc.pre_obs = None\n    dtc.ampl = dtc.protocols[1]['injected_square_current']['amplitude']*3.0\n    dtc = just_allen_predictions(dtc)\n    return dtc\nbagged = db.from_sequence(RAW_dtc['Neocortex pyramidal cell layer 5-6'])\n\nnewer_pop = list(bagged.map(cell_to_allen).compute())\n#print(RAW_dtc\n    #i.preds"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "preds = newer_pop[1][0].preds\n#preds"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "newer_pop[1][0].preds.pop('spikes',None)\npreds = {k:v['mean'] for k,v in newer_pop[1][0].preds.items()}\npreds\nallen_df = pd.DataFrame([preds])\nallen_df.T\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "druck1\ndruck1 = druck1.rename(index={0: str('ADEXP')})\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "druck4 = pd.DataFrame([izhidfeatures[-1]])\ndruck4 = druck4.rename(index={0: str('Izhi_last')})"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "druck3 = pd.DataFrame([dfeatures[-1]])\ndruck3 = druck3.rename(index={0: str('ADEXP_last')})\n#druck3.T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "druck2 = druck1\ndruck2 = druck2.append(druck0)\ndruck2 = druck2.append(druck3)\ndruck2 = druck2.append(druck4)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "#results['Cerebellum Purkinje cell_RAW'] \ncwd = os.getcwd()\nimport numpy as np\n# Open the 1.5x rheobase file\nfilename = os.path.join(cwd,'onefive_df.pkl')\nwith open(filename, 'rb') as f:\n    df = pickle.load(f)\n    \ndf.iloc[0][192:195]\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "import numpy as np\n# A function to convert all cells containing array (or other things) into floats.  \ndef f(x):\n    try:\n        return np.mean(x)\n    except:\n        try:\n            return np.mean(x['pred'])\n        except:\n            print(x)\n            raise e\ndruck2 = druck2.fillna(0).applymap(f)\n#d2 = druck2.fillna(0).applymap(f)\n\n\n#druck2.T\ndruck2\n\nfor col in druck2.columns:\n    for dm in df.columns:\n        if col in dm:\n            #print(col,dm)\n            #col = dm\n            #print(druck2[dm])\n            temp=druck2.rename(columns={col:dm},inplace=True)\ndruck2\n#temp\nbf = len(df)\ndf = df.append(druck2)\nat = len(df)\n\nsubset = df[druck2.columns]\nsubset.append(druck2)\ndf = subset\n\nnew_models_idx = list(range(bf,at,1))\nnew_models_df = df[df.index.isin(new_models_idx)]\nnew_models_df\n#len(new_models_df)\nnew_models_idx\nprint(bf,at)\nnew_models_df\nnew_models_idx\ndf\nnew = pd.concat([df.tail(4)])\nnew"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "## Use Random Forests, variance Explained.\n## Then Throw Away Feature Dimensions\nThrow that maximally seperate experimental cells, and models. This is done by using random forests to find features that explain the most variance.\n\nThe reason for this, is for each dimension we through away helps us deduce if that variance explained by that feature difference was crucial for seperating experimental data from models. In other words, we can identify weaker aspects  of models, and better target improvement areas of model performance.\n\nPrevious work has suggested models fail to mimic experimental cells in all the mannerisms pertaininig to features deleted in this data frame below."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "\n# in order to find out what is seperating and what is not.\n\ntry:\n    del df['InputResistanceTest_1.5x']\n    del df['InputResistanceTest_3.0x']\n\n    del df['ohmic_input_resistance_1.5x']\n    del df['ohmic_input_resistance_3.0x']\n    del df['time_1.5x']                              \n    #       0.190362\n    del df['decay_time_constant_after_stim_3.0x']\n    del df['voltage_deflection_3.0x']\n    del df['steady_state_hyper_3.0x']\n    del df['steady_state_voltage_stimend_3.0x']\n    del df['voltage_deflection_vb_ssse_3.0x']\n    del df['sag_amplitude_3.0x']\n    #0.198310\n    del df['is_not_stuck_1.5x']\nexcept:\n    print('features allready deleted.')\n\n\n# Apply this function to each dataframe in order to convert all cells into floats.\n# Also call fillna() first to impute missing values with 0's.  \ndf = df.fillna(0).applymap(f)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "druck2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "'''\ndf = df.append(druck2)\ndf.columns\ndf\nlist_cols=[]\nfor col in df.columns:\n    for j in druck2.columns:\n        if j in col:\n            for i in range(0,len(druck2)):\n\n                df.append(pd.Series(), ignore_index=True)\n\n                df.ix[len(df)-1, col] = druck2.ix[i,j]\n                #print(df.ix[len(df)-1, col])\n                #print(druck2.ix[i,j])\n                #print(df.ix[-1, col])\n                #print(col)\n                list_cols.append(col)\n#for col in list_cols:                \n#    print(df.ix[-1,col])            \n#df\ndf.ix[len(df)-1, col]\nprint(col)\ndf.columns\n'''"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "druck2\n#print(\"There are %d models+data and %d features\" % df.shape)\n# Special stuff to import\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import Isomap, TSNE\n#druck2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "# Turn all features into Normal(0,1) variables\n# Important since features all have different scales\nfrom sklearn.preprocessing import StandardScaler#, PCA\nss = StandardScaler()\ndruck2[:] = ss.fit_transform(druck2.values)\ndruck2.groupby(druck2.index).first()\ndruck2 = pd.DataFrame.drop_duplicates(druck2)\nprint(len(druck2))\ndf = druck2\n\n#df.fillna(0).applymap(f)\n\n#new_models_idx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "# Check to see what is there.  Might also check to see if there is data there.\n#df_30x#.head()\n\n# make model dataframe\nmodel_idx = [idx for idx in df.index.values if type(idx)==str]\nmodel_no_trans_df = df[df.index.isin(model_idx)]\nmodel_no_trans_df.index.name = 'Cell_ID'\nmodel_df = model_no_trans_df.copy()\nmodel_df.index.name = 'Cell_ID'\n\n# make experiment dataframe\nexperiment_idx = [idx for idx in df.index.values if type(idx)==int]\nexperiment_no_trans_df = df[df.index.isin(experiment_idx)]\nexperiment_df = experiment_no_trans_df.copy()\nprint(len(experiment_df))\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "#print(len(experiment_df))\nexperiment_df = pd.DataFrame.drop_duplicates(experiment_df)\n#experiment_df.index\nexperiment_df.groupby(experiment_df.index).first()\nlen(experiment_df)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "print(len(model_df))\nmodel_df.append(new)\nmodel_df = pd.DataFrame.drop_duplicates(model_df)\nmodel_df.index\nmodel_df.groupby(model_df.index).first()\n\nlen(model_df)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "from sklearn.decomposition import PCA\n\n# Do PCA and look at variance explained\npca = PCA()\npca.fit(df.T.values)\nn_features = df.shape[1]\nprint(n_features)\nprint(len(pca.explained_variance_ratio_.cumsum()))\nplt.plot(range(1,n_features-1),pca.explained_variance_ratio_.cumsum())\nplt.xlim(0,50);\nfeatures = df.columns\nplt.xticks()\nplt.xlabel('Number of principal components')\nplt.ylabel('Fraction of variance explained');"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "# THe transformed values, ordered from highest to lowest variance dimensions\ntransformed = pca.transform(df.values.T)\n\n\n#model_idx = [idx for idx in enumerate(df.index.values) if type(idx)==str]\n'''\n#label_model_no_trans_df\nmodel_no_trans_df.index.name = 'Cell_ID'\nmodel_df = model_no_trans_df.copy()\nmodel_df.index.name = 'Cell_ID'\n'''\n# make experiment dataframe\nexperiment_idx = [idx for idx in df.index.values if type(idx)==int]\nmodel_no_trans_df = df[~df.index.isin(experiment_idx)]\nexperiment_idx_labels = [(i,idx) for i,idx in enumerate(df.index.values) if type(idx)==int]\n\n#model_df\n#df.labels\nmodel_no_trans_df\nexperiment_idx_labels = [i[0] for i in experiment_idx_labels]\nexperiment_idx_labels\nmodel_no_trans_df\nmodel_index_labels = ~df.index.isin(experiment_idx)\n\nmodel_index_labels\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "# Do an isomap embedding in 2 dimensions\nisomap = Isomap(n_components=2)\nisomap.fit(df.values)\niso = isomap.embedding_.T\n# Plot that isomap embedding. Each is a model (or a cell, for data)\n#plt.scatter(iso);\nplt.clf()\nfig = plt.figure()\nax = plt.subplot(111)\nplt.scatter(iso[0,experiment_idx_labels],iso[1,experiment_idx_labels],c='blue',cmap='rainbow',label='data')\nplt.scatter(iso[0,model_index_labels],iso[1,model_index_labels],c='red',cmap='rainbow',label='models')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "standard = 1.5\nstrong = 3.0\neasy_map = [\n            {'AP12AmplitudeDropTest':standard},\n            {'AP1SSAmplitudeChangeTest':standard},\n            {'AP1AmplitudeTest':standard},\n            {'AP1WidthHalfHeightTest':standard},\n            {'AP1WidthPeakToTroughTest':standard},\n            {'AP1RateOfChangePeakToTroughTest':standard},\n            {'AP1AHPDepthTest':standard},\n            {'AP2AmplitudeTest':standard},\n            {'AP2WidthHalfHeightTest':standard},\n            {'AP2WidthPeakToTroughTest':standard},\n            {'AP2RateOfChangePeakToTroughTest':standard},\n            {'AP2AHPDepthTest':standard},\n            {'AP12AmplitudeChangePercentTest':standard},\n            {'AP12HalfWidthChangePercentTest':standard},\n            {'AP12RateOfChangePeakToTroughPercentChangeTest':standard},\n            {'AP12AHPDepthPercentChangeTest':standard},\n            {'InputResistanceTest':str('ir_currents')},\n            {'AP1DelayMeanTest':standard},\n            {'AP1DelaySDTest':standard},\n            {'AP2DelayMeanTest':standard},\n            {'AP2DelaySDTest':standard},\n            {'Burst1ISIMeanTest':standard},\n            {'Burst1ISISDTest':standard},\n            {'InitialAccommodationMeanTest':standard},\n            {'SSAccommodationMeanTest':standard},\n            {'AccommodationRateToSSTest':standard},\n            {'AccommodationAtSSMeanTest':standard},\n            {'AccommodationRateMeanAtSSTest':standard},\n            {'ISICVTest':standard},\n            {'ISIMedianTest':standard},\n            {'ISIBurstMeanChangeTest':standard},\n            {'SpikeRateStrongStimTest':strong},\n            {'AP1DelayMeanStrongStimTest':strong},\n            {'AP1DelaySDStrongStimTest':strong},\n            {'AP2DelayMeanStrongStimTest':strong},\n            {'AP2DelaySDStrongStimTest':strong},\n            {'Burst1ISIMeanStrongStimTest':strong},\n            {'Burst1ISISDStrongStimTest':strong},\n        ]\n\ndm_labels = [list(keys.keys())[0]+str('_')+str(list(keys.values())[0])+str('x') for keys in easy_map ]\n#dm_labels['AHP_depth_abs_slow_1.5x']\n\n#dir()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "\ndruck2.columns        \n#df.append(druck2)\ndruck2\n#df.columns"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "druck2.columns\ndf.append(druck2)\ndf['AP12AmplitudeDropTest_1.5x']\ndruck2['AP12AmplitudeDropTest_1.5x']"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": "df['AP12AmplitudeDropTest_1.5x']"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
      },
      "outputs": [
      ],
      "source": ""
    },
    {
      "cell_type": "raw",
      "metadata": {
      },
      "source": ""
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}