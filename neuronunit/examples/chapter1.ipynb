{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Below we perform a sparse grid sampling over the parameter space, using the published and well corrobarated parameter points, from Izhikitch publications, and the Open Source brain, shows that without optimization, using off the shelf parameter sets to fit real-life biological cell data, does not work so well.\n",
    "\n",
    "\n",
    "# Chapter 1.\n",
    "In this chapter we will look at how the sciunit optimizer is readily used to fit abstract neuronal model behavior to experimentally recorded neuron waveform shapes. You can think of this problem as a type of inverse search, where we have experimental data that can be used to constrain a waveform shape. There are lots of different possible parameterizations of models we will explore, and we want to select the particular model parameterization that best agrees with experimentally recorded measurements. \n",
    "\n",
    "Golowasch, J., Goldman, M., Abbott, L.F, and Marder, E. (2002)\n",
    "Failure of averaging in the construction\n",
    "of conductance-based neuron models. J. Neurophysiol., 87: 11291131.\n",
    "\n",
    "### Next Chapters\n",
    "In **chapter 2**  We demonstrate optimization using spike time statistics via the allen SDK\n",
    "[Chapter 2](chapter2.ipynb)\n",
    "\n",
    "In **chapter 3**  We will take a closer at the data used to perform the fits in this notebook.\n",
    "[Chapter 3](chapter3.ipynb)\n",
    "\n",
    "In **chapter 5** We will look at projections of Optimized cells onto a Druckman feature space, we will also look at extracting Allen SDK features from the optimized cells.\n",
    "[Chapter 3](chapter5.ipynb)\n",
    "\n",
    "In **chapter 8** We look at building ***neuronunit*** modal data tests from neuroelectro data.\n",
    "[Chapter 9](chapter8.ipynb)\n",
    "\n",
    "In **chapter 9** We look at class relationships, optimizing syntax, and an exhaustive search approach to ground truths.\n",
    "[Chapter 9](chapter9.ipynb)\n",
    "\n",
    "The table below includes the five different varieties of experimental cell data that we will demonstrate how to optimize against. We will find solutions to 10 different problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why are we doing this.\n",
    "As data accumulates we are learning that many models are not good fits to experimental data.\n",
    "We can use optimization to find better or more finely grained data specific fits of model parameters.\n",
    "\n",
    "For example consider the table below which shows model performance over 8 different testing criterion. In this table scores of 8 are bad and scores of 0 are good.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consider installing pynn a heavier backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO       Cache size for target \"cython\": 4765 MB.\n",
      "You can call \"clear_cache('cython')\" to delete all files from the cache or manually delete files in the \"/home/russell/.cython/brian_extensions\" directory. [brian2]\n",
      "WARNING    /home/russell/safe2/neuronunit/neuronunit/optimisation/optimization_management.py:8: UserWarning: X11 plotting backend not available, consider installing\n",
      "  warnings.warn('X11 plotting backend not available, consider installing')\n",
      " [py.warnings]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glif python Error\n",
      "/home/russell/safe2/neuronunit/neuronunit/tests/multicellular_constraints.p\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ef61e507d197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0melectro_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/../tests/multicellular_constraints.p'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melectro_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "from neuronunit.optimisation import model_parameters\n",
    "from neuronunit.optimisation.data_transport_container import DataTC\n",
    "from neuronunit.optimisation.optimization_management import dtc_to_rheo#, format_test\n",
    "from sciunit.scores.collections import ScoreMatrix#(pd.DataFrame, SciUnit, TestWeighted)\n",
    "known_parameters = model_parameters.reduced_cells\n",
    "MODEL_PARAMS = model_parameters.MODEL_PARAMS\n",
    "import pandas as pd\n",
    "from neuronunit.optimisation import mint_tests\n",
    "from neuronunit.optimisation.optimization_management import OptMan, TSD\n",
    "import numpy as np\n",
    "\n",
    "electro_path = str(os.getcwd())+'/../tests/multicellular_constraints.p'\n",
    "try:\n",
    "    assert os.path.isfile(electro_path) == True\n",
    "    with open(electro_path,'rb') as f:\n",
    "        test_frame = pickle.load(f)\n",
    "    filtered_tests = {key:val for key,val in test_frame.items()}\n",
    "except:\n",
    "    \n",
    "    filtered_tests = mint_tests.get_cell_constraints()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_frame = filtered_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_known_params(test_frame,backend,known_parameters):\n",
    "    df = pd.DataFrame(index=list(test_frame.keys()),columns=list(known_parameters.keys()))\n",
    "\n",
    "    for l,(key, use_test) in enumerate(test_frame.items()):\n",
    "        use_test = TSD(use_test)\n",
    "        use_test.use_rheobase_score\n",
    "\n",
    "        OM = OptMan(use_test,backend=backend,\\\n",
    "                    boundary_dict=MODEL_PARAMS[backend],\\\n",
    "                    protocol={'elephant':True,'allen':False,'dm':False},)#'tsr':spk_range})\n",
    "        dtcpop = []\n",
    "        for k,v in known_parameters.items():\n",
    "            use_test = TSD(use_test)\n",
    "            use_test.use_rheobase_score\n",
    "            temp = {}\n",
    "            temp[str(v)] = {}\n",
    "            dtc = DataTC()\n",
    "            dtc.attrs = v\n",
    "            dtc.backend = backend\n",
    "            dtc.cell_name = 'vanilla'\n",
    "            dtc.tests = use_test\n",
    "            dtc = dtc_to_rheo(dtc)\n",
    "            #print(dtc.rheobase)\n",
    "            dtc.tests = dtc.format_test()\n",
    "            dtcpop.append(dtc)\n",
    "        dtcpop = list(map(OM.elephant_evaluation,dtcpop))\n",
    "        for i,j in enumerate(dtcpop):\n",
    "            # TODO use the sciunit collections container.\n",
    "            df.iloc[l][i] = np.sum(list(j.scores.values()))/len(list(j.scores.values()))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "#dfi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dfi = pickle.load(open('known_table.p','rb'))\n",
    "except:\n",
    "    dfi = get_known_params(test_frame,str('RAW'),known_parameters)\n",
    "    pickle.dump(dfi,open('known_table.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RS</th>\n",
       "      <th>IB</th>\n",
       "      <th>LTS</th>\n",
       "      <th>TC</th>\n",
       "      <th>TC_burst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Olfactory bulb (main) mitral cell</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hippocampus CA1 pyramidal cell</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cerebellum Purkinje cell</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neocortex pyramidal cell layer 5-6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     RS   IB  LTS   TC TC_burst\n",
       "Olfactory bulb (main) mitral cell   NaN  NaN  NaN  NaN      NaN\n",
       "Hippocampus CA1 pyramidal cell      NaN  NaN  NaN  NaN      NaN\n",
       "Cerebellum Purkinje cell            NaN  NaN  NaN  NaN      NaN\n",
       "Neocortex pyramidal cell layer 5-6  NaN  NaN  NaN  NaN      NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuronunit.optimisation import model_parameters\n",
    "from neuronunit.optimisation.data_transport_container import DataTC\n",
    "from neuronunit.optimisation.optimization_management import dtc_to_rheo#, format_test\n",
    "\n",
    "reduced_cells = model_parameters.reduced_cells\n",
    "df = pd.DataFrame(index=list(test_frame.keys()),columns=list(reduced_cells.keys()))\n",
    "electro_path = str(os.getcwd())+'/../tests/russell_tests.p'\n",
    "\n",
    "assert os.path.isfile(electro_path) == True\n",
    "with open(electro_path,'rb') as f:\n",
    "    (test_frame,obs_frame) = pickle.load(f)\n",
    "filtered_tests = {key:val for key,val in test_frame.items()}\n",
    "\n",
    "# print('exceptional circumstances pickle file does not exist, rebuilding sparse grid for Izhikich')\n",
    "# Below we perform a sparse grid sampling over the parameter space, using the published and well corrobarated parameter points, from Izhikitch publications, and the Open Source brain, shows that without optimization, using off the shelf parameter sets to fit real-life biological cell data, does not work so well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| Cell Type   |      Model Type1       |     Model Type2     | simulator backend |\n",
    "|----------|----------|----------|:-------------:|\n",
    "| Cerebellum Purkinje cell   | Adaptive Exponential | Izhikitich Model   |brian2,Forward Euler|\n",
    "| Olfactory bulb (main) mitral cell |    Adaptive Exponential | Izhikitich Model   |brian2,Forward Euler|\n",
    "| Hippocampus CA1 pyramidal cell\t | Adaptive Exponential | Izhikitich Model |brian2,Forward Euler|\n",
    "| Neocortex pyramidal cell layer 5-6 | Adaptive Exponential | Izhikitich Model|brian2,Forward Euler|\n",
    "| Hippocampus CA1 basket cell | Adaptive Exponential | Izhikitich Model|brian2,Forward Euler|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get optimization results.\n",
    "In the cells below we either preload pre-optimized data for five different experimental cell types, ir in the absence of data, do the optimization in place below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up environment.\n",
    "In the cell below we set up an environment that supports visualization of \n",
    "pre-computed optimization results. This also includes download of the results. This also includes forcing a notebook compliant plotting backend initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from neuronunit.optimisation.optimization_management import TSD, get_dtc_pop\n",
    "from neuronunit.optimisation import model_parameters\n",
    "\n",
    "\n",
    "try:\n",
    "    import nbimporter\n",
    "except:\n",
    "    !pip install nbimporter\n",
    "    import nbimporter\n",
    "from importlib import reload\n",
    "import chapter5 as chapter5\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import efel\n",
    "    import tabulate\n",
    "\n",
    "except:\n",
    "    !pip install efel\n",
    "    !pip install tabulate\n",
    "    \n",
    "## ToDo make this a dictionary hashing caching system that is wraped in an opt-man object.\n",
    "from neuronunit.optimisation.get_three_feature_sets_from_nml_db import three_feature_sets_on_static_models\n",
    "try:\n",
    "    results = pickle.load(open('../unit_test/working/all_data_tests.p','rb'))\n",
    "except:\n",
    "    try:\n",
    "        os.system('wget https://www.dropbox.com/s/cod7jz4yrr55dsw/all_data_tests.p?dl=0')\n",
    "        results = pickle.load(open('../unit_test/working/all_data_tests.p?dl=0','rb'))\n",
    "    except:\n",
    "        import elephant_data_tests\n",
    "\n",
    "        # No data available, so lets generate data in place below:\n",
    "        # Do the optization in place.\n",
    "        results = pickle.load(open('../unit_test/working/all_data_tests.p','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in pre-wrangled/refined data\n",
    "that was output from a previous optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "electro_path = str(os.getcwd())+'/../tests/russell_tests.p'\n",
    "\n",
    "assert os.path.isfile(electro_path) == True\n",
    "with open(electro_path,'rb') as f:\n",
    "    (test_frame,obs_frame) = pickle.load(f)\n",
    "\"\"\"   \n",
    "filtered_tests = {key:val for key,val in test_frame.items()}\n",
    "\n",
    "print('Neuronunit tests used to constrain models against these experimental cell types {0}'.format(filtered_tests.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "result_RAW = pickle.load(open('RAWall_data_tests.p','rb'))\n",
    "result_ADEXP = pickle.load(open('ADEXPall_data_tests.p','rb'))\n",
    "result_RAW = result_RAW['RAW']\n",
    "ad_olf_dtc = result_ADEXP['ADEXP']['olf'][0].dtc\n",
    "ad_purkine_dtc = result_ADEXP['ADEXP']['purkine'][0].dtc\n",
    "ad_ca1pyr_dtc = result_ADEXP['ADEXP']['ca1pyr'][0].dtc\n",
    "ad_ca1basket_dtc = result_ADEXP['ADEXP']['ca1basket'][0].dtc\n",
    "ad_neo_dtc = result_ADEXP['ADEXP']['neo'][0].dtc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olf_dtc = result_RAW['olf'][0]\n",
    "purkine_dtc = result_RAW['purkine'][0]\n",
    "ca1pyr_dtc = result_RAW['ca1pyr'][0]\n",
    "ca1basket_dtc = result_RAW['ca1basket'][0]\n",
    "neo_dtc = result_RAW['neo'][0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(neo_dtc)\n",
    "neo_dtc.scores_ratio\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import nbimporter\n",
    "except:\n",
    "    !pip install nbimporter\n",
    "    import nbimporter\n",
    "from importlib import reload\n",
    "import chapter5 as chapter5\n",
    "\n",
    "#reload(chapter5)\n",
    "to_df = []\n",
    "ADEXP_dtc,RAW_dtc = chapter5.result_to_dict(result_ADEXP,result_RAW)\n",
    "for k in ADEXP_dtc.keys():\n",
    "    scores_ratio = {}\n",
    "    scores_ratio['forward_euler_izhikitich'] = RAW_dtc[k][0].scores_ratio\n",
    "    scores_ratio['brian2_adaptive_exponential'] = ADEXP_dtc[k][0].scores_ratio\n",
    "    to_df.append(scores_ratio)\n",
    "dfsr = pd.DataFrame(to_df)    \n",
    "for i,k in enumerate(ADEXP_dtc.keys()):\n",
    "    dfsr = dfsr.rename(index={i:k})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data frame below you can see that the Hippocampus CA1 basket cell was the most able to impose it's constraints onto abstract models. Inhibitory neurons are often fast spiking. The mitral olfactory bulb data was also highly amenable to optimization in the case of the apative exponential model, however we should not be too surprised as the corresponding ***Neuronunit*** test suite consisted of fewer constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data frame below follows this pattern:\n",
    "\n",
    "| Cell Type   |       Adaptive Exponential      |     Izhikitich Model    |\n",
    "|----------|----------|:-------------:|\n",
    "| Cerebellum Purkinje cell   | ratio score 0-1, over 8 tests, lower is better |  ratio score 0-1, over 8 tests, lower is better |\n",
    "| Olfactory bulb (main) mitral cell | ratio score 0-1, over 8 tests, lower is better | ratio score 0-1, over 8 tests, lower is better   |\n",
    "| Hippocampus CA1 pyramidal cell\t | ratio score 0-1, over 8 tests, lower is better | ratio score 0-1, over 8 tests, lower is better|\n",
    "| Neocortex pyramidal cell layer 5-6 | ratio score 0-1, over 8 tests, lower is better | ratio score 0-1, over 8 tests, lower is better |\n",
    "| Hippocampus CA1 basket cell | ratio score 0-1, over 8 tests, lower is better | ratio score 0-1, over 8 tests, lower is better|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(dfsr['forward_euler_izhikitich'])   \n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi['optimized'] = sub\n",
    "dfi\n",
    "\n",
    "\n",
    "def _color_red_or_green(val):\n",
    "    color = 'red' if val > 0.5 else 'green'\n",
    "    return 'color: %s' % color\n",
    "dfi = dfi.style.applymap(_color_red_or_green)\n",
    "\n",
    "dfi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We picture the CA1 Basket Cell spike waveform shape below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuronunit.optimisation.optimization_management import inject_and_plot\n",
    "\n",
    "_ = inject_and_plot([ca1basket_dtc],second_pop=[ad_ca1basket_dtc],third_pop=[ad_ca1basket_dtc],snippets=True,experimental_cell_type='CA1 Basket Cell')\n",
    "_ = inject_and_plot([ca1basket_dtc],second_pop=[ad_ca1basket_dtc],third_pop=[ad_ca1basket_dtc],snippets=False,experimental_cell_type='CA1 Basket Cell')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuronunit.optimisation.optimization_management import transform, save_models_for_justas, write_opt_to_nml\n",
    "try:\n",
    "    write_opt_to_nml(olf_dtc)\n",
    "    save_models_for_justas(olf_dtc)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Iterated Plots Below:\n",
    "For every model pertaining to a different experimental cell show the rheobase spike waveform shape for the best  solution from the two different classes of optimized models. This is an indirect way of cross checking that optimizer worked, as it exerted the same constraints on different neural models, therefore one would expect to see similar waveforms in the same plot, just with different colors.\n",
    "\n",
    "Initially you can see that spike onset time was not a feature used to constrain models, therefore the two different model classes vary a lot in spike onset time, however, we were less interested in spike timing, and more interested in waveform shape properties. Therefore in second form of plots (scroll down), one can see that spike onset time has been artificially controlled for in the spike visualization be re-aligning waveforms.\n",
    "\n",
    "The adpative exponential model has an artificial triangular appearance, only because as model developers we realized we could further optimize the cells experimental agreement, by adding in code hacks to improve the cells spike amplitude.\n",
    "\n",
    "Without this code modification, waveform shapes look deceptively dissimilar to a human viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_ = inject_and_plot([olf_dtc],second_pop=[ad_olf_dtc],third_pop=[ad_olf_dtc],snippets=True,experimental_cell_type='Olfactory Mitral Cell')\n",
    "_ = inject_and_plot([ca1basket_dtc],second_pop=[ad_ca1basket_dtc],third_pop=[ad_ca1basket_dtc],snippets=True,experimental_cell_type='CA1 Basket Cell')\n",
    "_ = inject_and_plot([neo_dtc],second_pop=[ad_neo_dtc],third_pop=[ad_neo_dtc],snippets=True,experimental_cell_type='Neo Cortical Pyramidal Cell')\n",
    "_ = inject_and_plot([ca1pyr_dtc],second_pop=[ad_ca1pyr_dtc],third_pop=[ad_ca1pyr_dtc],snippets=True,experimental_cell_type='CA1 Cortical Pyramidal Cell')\n",
    "_ = inject_and_plot([purkine_dtc],second_pop=[ad_purkine_dtc],third_pop=[ad_purkine_dtc],snippets=True,experimental_cell_type='Cerebellar Purkinje Cell')\n",
    "\n",
    "_ = inject_and_plot([olf_dtc],second_pop=[ad_olf_dtc],third_pop=[ad_olf_dtc],experimental_cell_type='Olfactory Mitral Cell')\n",
    "_ = inject_and_plot([ca1basket_dtc],second_pop=[ad_ca1basket_dtc],third_pop=[ad_ca1basket_dtc],experimental_cell_type='CA1 Basket Cell')\n",
    "_ = inject_and_plot([neo_dtc],second_pop=[ad_neo_dtc],third_pop=[ad_neo_dtc],experimental_cell_type='Neo Cortical Pyramidal Cell')\n",
    "_ = inject_and_plot([ca1pyr_dtc],second_pop=[ad_ca1pyr_dtc],third_pop=[ad_ca1pyr_dtc],experimental_cell_type='CA! Cortical Pyramidal Cell')\n",
    "_ = inject_and_plot([purkine_dtc],second_pop=[ad_purkine_dtc],third_pop=[ad_purkine_dtc],experimental_cell_type='Cerebellar Purkinje Cell')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAW.keys()\n",
    "#ADEXP.keys()\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([ad_purkine_dtc.attrs])\n",
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Diversity of Optimization solution sets:\n",
    "### Plot all optimized cells from the pareto front fo just one class of cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this is to show that the pareto front from the converged genetic algorithm, retains much important variation in spike waveform shape.\n",
    "\n",
    "Diversity of firing shape is retained in the NSGA3 algorithm, by deliberatley favoring collections of solutions, consisting of vary different parameter sets (solution hyper volume is maximized as one of many optimization criterion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for k,v in ADEXP_dtc.items():\n",
    "    vm = inject_and_plot(RAW_dtc[k],second_pop=ADEXP_dtc[k],experimental_cell_type=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in ADEXP_dtc.items():\n",
    "\n",
    "    vm = inject_and_plot(RAW_dtc[k],second_pop=ADEXP_dtc[k],snippets=True,experimental_cell_type=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Do a cursory check of Neuron Unit test scores.\n",
    "by looking into observation/prediction agreement for the two different models classes, in the case of optimizing against one particular experimental cell. In priciple this can easily be done for all of the different experimental cells.\n",
    "\n",
    "Note many of the tables below, are unintentional clones of each other. This is an intermittent problem to do with indexing data properly that I am currently fixing. By examining the graphs above its easy to confirm that optimization outputs appropriately vary to match specific cells.\n",
    "\n",
    "We will more thoroughly interrogate test scores in chapter3, as this work is continued in detail in [chapter3](chapter3.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(result_ADEXP.keys())\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "import copy\n",
    "RAW_dtc = {}\n",
    "ADEXP_dtc = {}\n",
    "#type(result_ADEXP['ADEXP']['olf'][0])\n",
    "RAW_dtc = {}\n",
    "RAW_dtc['Cerebellum Purkinje cell'] = result_RAW['purkine']\n",
    "RAW_dtc['Olfactory bulb (main) mitral cell'] = result_RAW['olf']\n",
    "RAW_dtc['Hippocampus CA1 pyramidal cell'] = result_RAW['ca1pyr']\n",
    "RAW_dtc['Neocortex pyramidal cell layer 5-6'] = result_RAW['neo']\n",
    "RAW_dtc['Hippocampus CA1 basket cell'] = result_RAW['ca1basket']\n",
    "\n",
    "ADEXP_dtc = {}\n",
    "ADEXP_dtc['Cerebellum Purkinje cell'] = [d.dtc for d in result_ADEXP['ADEXP']['purkine']]\n",
    "ADEXP_dtc['Olfactory bulb (main) mitral cell'] = [d.dtc for d in result_ADEXP['ADEXP']['olf']]\n",
    "ADEXP_dtc['Hippocampus CA1 pyramidal cell'] = [d.dtc for d in result_ADEXP['ADEXP']['ca1pyr']]\n",
    "ADEXP_dtc['Neocortex pyramidal cell layer 5-6'] = [d.dtc for d in result_ADEXP['ADEXP']['neo']]\n",
    "ADEXP_dtc['Hippocampus CA1 basket cell'] = [d.dtc for d in result_ADEXP['ADEXP']['ca1basket']]\n",
    "\n",
    "for k in RAW_dtc.keys():\n",
    "    print(k)\n",
    "    df1 = None\n",
    "    df0 = None\n",
    "    df1 = pd.DataFrame([RAW_dtc[k][0].scores])\n",
    "    df1= df1.rename(index={0: str('Izhikivitch')})\n",
    "    df0 = pd.DataFrame([ADEXP_dtc[k][0].scores])\n",
    "    df0 = df0.rename(index={0: str('ADEXP')})\n",
    "    df2 = df0.append(df1)\n",
    "    df3 = df2.T\n",
    "    display(df3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "md = tabulate.tabulate(df3, headers='keys', tablefmt='pipe')\n",
    "# Fix the markdown string; it will not render with an empty first table cell,\n",
    "# so if the dataframe's index has no name, just place an 'x' there.\n",
    "md = md.replace('|    |','| %s |' % (df.index.name if df.index.name else 'x'))\n",
    "\n",
    "from IPython.display import display, Markdown, Latex\n",
    "display(Markdown(md))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above it looks like the adaptive expontial model struggles to recaptiluate an experimental value of a rheobase test current injection, but does well on most other tests. It even outperforms the izhikitch model on Injected Amplitude accuracy, Input resistance accuracy, Resting potenial accuracy, and the time constant test.\n",
    "\n",
    "The izhikevitch model has a different strength weekness profile to the Adaptive Exponential model. The izhikitch model is unable to recapitulate  experimental data for the input resistance test, but it does well better than the adaptive expontial model at matching the experimental capacitance, and matching the experimental width to name a few scores. The adaptive exponential model is able to do well at matching experimental rheobase values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In chapter 3\n",
    "We will go beyond Neuronunits optimization test scores and instead check for agreement between the experimental literature and model outputs.\n",
    "\n",
    "\n",
    "In **chapter 3** (a closely related notebook see [**chapter 3**](chapter3.ipynb) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The user of the Sciunit optimization can use the outuput of previous optimizations to initiate future optimization work. Optimization often involves an exploration/exploitation trade-off. The user of this optimization tool, can opt-into greater exploration of the solution space. See below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "%%capture\n",
    "from neuronunit.optimisation.optimisations import run_ga\n",
    "from neuronunit.optimisation import model_parameters\n",
    "MODEL_PARAMS=model_parameters.MODEL_PARAMS['ADEXP']\n",
    "test = TSD(filtered_tests['Hippocampus CA1 pyramidal cell'])\n",
    "test.use_rheobase_score = True\n",
    "out = run_ga(MODEL_PARAMS, 1, test, \\\n",
    "        free_params = MODEL_PARAMS.keys(), hc = None, MU = len(result_ADEXP['ADEXP']['ca1pyr']), seed_pop = result_ADEXP['ADEXP']['ca1pyr'], \\\n",
    "           backend = str('ADEXP'),protocol={'use_rheobase_score':True,'allen':False,'elephant':True})\n",
    "\n",
    "filtered_tests.keys()\n",
    "out\n",
    "```           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of chapter 1\n",
    "*** see chapters 2,3,4,5 ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disregard the following content which is a draft copy of chapter 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare0 = {t.name:t.observation for t in RAW_dtc['Neocortex pyramidal cell layer 5-6'][0].tests.values() }\n",
    "compare0 = {k:(v,RAW_dtc['Neocortex pyramidal cell layer 5-6'][0].predictions[k]) for k,v in compare0.items()  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([t for t in ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].tests ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare1 = {t.name:t.observation for t in ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].tests.values() }\n",
    "print(compare1.keys())\n",
    "print(ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].predictions.keys())\n",
    "compare1 = {k:(v,ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].predictions[k]) for k,v in compare1.items() if k in ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].predictions.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_nice_frame(compare0):\n",
    "    pre_df = {}\n",
    "    for k,v in compare0.items():\n",
    "        temp = v[0][list(v[0].keys())[0]].rescale(v[1][list(v[1].keys())[0]].units)\n",
    "        pre_df[k] = (temp,v[1][list(v[1].keys())[0]],v[0]['std'].rescale(v[1][list(v[1].keys())[0]].units))\n",
    "        df6=pd.DataFrame([pre_df])    \n",
    "\n",
    "    return df6  \n",
    "df6 = format_nice_frame(compare0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare observations and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df6.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at where the optimized cells reside in regular parameter space\n",
    "The NSGA3 algorithm returns a pareto front as a solution, this front as acts as a set of diverse model parameterizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_dtc['Neocortex pyramidal cell layer 5-6'][0].attrs.pop('Iext',None)\n",
    "attrsf0=pd.DataFrame([RAW_dtc['Neocortex pyramidal cell layer 5-6'][0].attrs])\n",
    "attrsf0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_dtc['Neocortex pyramidal cell layer 5-6'][-1].attrs.pop('Iext',None)\n",
    "attrsf1=pd.DataFrame([RAW_dtc['Neocortex pyramidal cell layer 5-6'][-1].attrs])\n",
    "attrsf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].attrs.pop('Iext',None)\n",
    "attrsf3=pd.DataFrame([ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].attrs])\n",
    "attrsf3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][0].attrs.pop('Iext',None)\n",
    "attrsf4=pd.DataFrame([ADEXP_dtc['Neocortex pyramidal cell layer 5-6'][-1].attrs])\n",
    "attrsf4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## In the notebook cells below we compute Druckman and Allen SDK\n",
    "Features on the optimized cells, in order to see if the optimized cells fall into pre-defined clusters, differentally  associated with experimental data, and model output data.\n",
    "\n",
    "To say this another way, it has previously been observed that experimental cell data, and model data, falls into easily seperated categories in feature space. This probably reflects deficits in simple model realism.\n",
    "\n",
    "By using optimized cells as new data points, and plotting their positions in a reduced dimension feature space we will be able to see if optimized models, are more convincing imitations of experimental data, by testing if new data points are harder to seperate from the experimental data.\n",
    "### Get Druckman features using a parallel algorithm to save time\n",
    "The Dask Bag data type allows you to map embarrassingly parallel functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "from neuronunit.optimisation.optimization_management import nuunit_dm_evaluation\n",
    "bagged = db.from_sequence(RAW_dtc['Neocortex pyramidal cell layer 5-6'])\n",
    "druckman_feature_coordinates_izhi = list(bagged.map(nuunit_dm_evaluation).compute())\n",
    "izhidfeatures = [d.dm_test_features for d in druckman_feature_coordinates_izhi ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck0 = pd.DataFrame([izhidfeatures[0]])\n",
    "druck0 = druck0.rename(index={0: str('Izhikivitch')})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.bag as db\n",
    "bagged = db.from_sequence(ADEXP_dtc['Neocortex pyramidal cell layer 5-6'])\n",
    "druckman_feature_coordinatesadexp = list(bagged.map(nuunit_dm_evaluation).compute())\n",
    "dfeatures = [d.dm_test_features for d in druckman_feature_coordinatesadexp ]\n",
    "druck1 = pd.DataFrame([dfeatures[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Allen features with a parallel algorithm.\n",
    "This is done by appropriating the code idiom above used in the Druckman case.\n",
    "\n",
    "using a parallel algorithm to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RAW_dtc = [d.dtc for d in RAW[k]]\n",
    "\n",
    "from neuronunit.optimisation.optimization_management import just_allen_predictions\n",
    "#RAW_dtc = [d.dtc for d in RAW[k]]\n",
    "RAW_dtc['Neocortex pyramidal cell layer 5-6'][0].protocols[1].keys()\n",
    "\n",
    "RAW_dtc['Neocortex pyramidal cell layer 5-6'][0].ampl = RAW_dtc['Neocortex pyramidal cell layer 5-6'][0].protocols[1]['injected_square_current']['amplitude']*1.5\n",
    "RAW_dtc['Neocortex pyramidal cell layer 5-6'][0].ampl\n",
    "#for i in RAW_dtc:\n",
    "    \n",
    "def cell_to_allen(dtc):    \n",
    "    dtc.pre_obs = None\n",
    "    dtc.ampl = dtc.protocols[1]['injected_square_current']['amplitude']*3.0\n",
    "    dtc = just_allen_predictions(dtc)\n",
    "    return dtc\n",
    "bagged = db.from_sequence(RAW_dtc['Neocortex pyramidal cell layer 5-6'])\n",
    "\n",
    "newer_pop = list(bagged.map(cell_to_allen).compute())\n",
    "#print(RAW_dtc\n",
    "    #i.preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = newer_pop[1][0].preds\n",
    "#preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newer_pop[1][0].preds.pop('spikes',None)\n",
    "preds = {k:v['mean'] for k,v in newer_pop[1][0].preds.items()}\n",
    "preds\n",
    "allen_df = pd.DataFrame([preds])\n",
    "allen_df.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck1\n",
    "druck1 = druck1.rename(index={0: str('ADEXP')})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck4 = pd.DataFrame([izhidfeatures[-1]])\n",
    "druck4 = druck4.rename(index={0: str('Izhi_last')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck3 = pd.DataFrame([dfeatures[-1]])\n",
    "druck3 = druck3.rename(index={0: str('ADEXP_last')})\n",
    "#druck3.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck2 = druck1\n",
    "druck2 = druck2.append(druck0)\n",
    "druck2 = druck2.append(druck3)\n",
    "druck2 = druck2.append(druck4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results['Cerebellum Purkinje cell_RAW'] \n",
    "cwd = os.getcwd()\n",
    "import numpy as np\n",
    "# Open the 1.5x rheobase file\n",
    "filename = os.path.join(cwd,'onefive_df.pkl')\n",
    "with open(filename, 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "    \n",
    "df.iloc[0][192:195]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# A function to convert all cells containing array (or other things) into floats.  \n",
    "def f(x):\n",
    "    try:\n",
    "        return np.mean(x)\n",
    "    except:\n",
    "        try:\n",
    "            return np.mean(x['pred'])\n",
    "        except:\n",
    "            print(x)\n",
    "            raise e\n",
    "druck2 = druck2.fillna(0).applymap(f)\n",
    "#d2 = druck2.fillna(0).applymap(f)\n",
    "\n",
    "\n",
    "#druck2.T\n",
    "druck2\n",
    "\n",
    "for col in druck2.columns:\n",
    "    for dm in df.columns:\n",
    "        if col in dm:\n",
    "            #print(col,dm)\n",
    "            #col = dm\n",
    "            #print(druck2[dm])\n",
    "            temp=druck2.rename(columns={col:dm},inplace=True)\n",
    "druck2\n",
    "#temp\n",
    "bf = len(df)\n",
    "df = df.append(druck2)\n",
    "at = len(df)\n",
    "\n",
    "subset = df[druck2.columns]\n",
    "subset.append(druck2)\n",
    "df = subset\n",
    "\n",
    "new_models_idx = list(range(bf,at,1))\n",
    "new_models_df = df[df.index.isin(new_models_idx)]\n",
    "new_models_df\n",
    "#len(new_models_df)\n",
    "new_models_idx\n",
    "print(bf,at)\n",
    "new_models_df\n",
    "new_models_idx\n",
    "df\n",
    "new = pd.concat([df.tail(4)])\n",
    "new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Random Forests, variance Explained.\n",
    "## Then Throw Away Feature Dimensions\n",
    "Throw that maximally seperate experimental cells, and models. This is done by using random forests to find features that explain the most variance.\n",
    "\n",
    "The reason for this, is for each dimension we through away helps us deduce if that variance explained by that feature difference was crucial for seperating experimental data from models. In other words, we can identify weaker aspects  of models, and better target improvement areas of model performance.\n",
    "\n",
    "Previous work has suggested models fail to mimic experimental cells in all the mannerisms pertaininig to features deleted in this data frame below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# in order to find out what is seperating and what is not.\n",
    "\n",
    "try:\n",
    "    del df['InputResistanceTest_1.5x']\n",
    "    del df['InputResistanceTest_3.0x']\n",
    "\n",
    "    del df['ohmic_input_resistance_1.5x']\n",
    "    del df['ohmic_input_resistance_3.0x']\n",
    "    del df['time_1.5x']                              \n",
    "    #       0.190362\n",
    "    del df['decay_time_constant_after_stim_3.0x']\n",
    "    del df['voltage_deflection_3.0x']\n",
    "    del df['steady_state_hyper_3.0x']\n",
    "    del df['steady_state_voltage_stimend_3.0x']\n",
    "    del df['voltage_deflection_vb_ssse_3.0x']\n",
    "    del df['sag_amplitude_3.0x']\n",
    "    #0.198310\n",
    "    del df['is_not_stuck_1.5x']\n",
    "except:\n",
    "    print('features allready deleted.')\n",
    "\n",
    "\n",
    "# Apply this function to each dataframe in order to convert all cells into floats.\n",
    "# Also call fillna() first to impute missing values with 0's.  \n",
    "df = df.fillna(0).applymap(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df = df.append(druck2)\n",
    "df.columns\n",
    "df\n",
    "list_cols=[]\n",
    "for col in df.columns:\n",
    "    for j in druck2.columns:\n",
    "        if j in col:\n",
    "            for i in range(0,len(druck2)):\n",
    "\n",
    "                df.append(pd.Series(), ignore_index=True)\n",
    "\n",
    "                df.ix[len(df)-1, col] = druck2.ix[i,j]\n",
    "                #print(df.ix[len(df)-1, col])\n",
    "                #print(druck2.ix[i,j])\n",
    "                #print(df.ix[-1, col])\n",
    "                #print(col)\n",
    "                list_cols.append(col)\n",
    "#for col in list_cols:                \n",
    "#    print(df.ix[-1,col])            \n",
    "#df\n",
    "df.ix[len(df)-1, col]\n",
    "print(col)\n",
    "df.columns\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck2\n",
    "#print(\"There are %d models+data and %d features\" % df.shape)\n",
    "# Special stuff to import\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "#druck2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn all features into Normal(0,1) variables\n",
    "# Important since features all have different scales\n",
    "from sklearn.preprocessing import StandardScaler#, PCA\n",
    "ss = StandardScaler()\n",
    "druck2[:] = ss.fit_transform(druck2.values)\n",
    "druck2.groupby(druck2.index).first()\n",
    "druck2 = pd.DataFrame.drop_duplicates(druck2)\n",
    "print(len(druck2))\n",
    "df = druck2\n",
    "\n",
    "#df.fillna(0).applymap(f)\n",
    "\n",
    "#new_models_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see what is there.  Might also check to see if there is data there.\n",
    "#df_30x#.head()\n",
    "\n",
    "# make model dataframe\n",
    "model_idx = [idx for idx in df.index.values if type(idx)==str]\n",
    "model_no_trans_df = df[df.index.isin(model_idx)]\n",
    "model_no_trans_df.index.name = 'Cell_ID'\n",
    "model_df = model_no_trans_df.copy()\n",
    "model_df.index.name = 'Cell_ID'\n",
    "\n",
    "# make experiment dataframe\n",
    "experiment_idx = [idx for idx in df.index.values if type(idx)==int]\n",
    "experiment_no_trans_df = df[df.index.isin(experiment_idx)]\n",
    "experiment_df = experiment_no_trans_df.copy()\n",
    "print(len(experiment_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(experiment_df))\n",
    "experiment_df = pd.DataFrame.drop_duplicates(experiment_df)\n",
    "#experiment_df.index\n",
    "experiment_df.groupby(experiment_df.index).first()\n",
    "len(experiment_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(model_df))\n",
    "model_df.append(new)\n",
    "model_df = pd.DataFrame.drop_duplicates(model_df)\n",
    "model_df.index\n",
    "model_df.groupby(model_df.index).first()\n",
    "\n",
    "len(model_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Do PCA and look at variance explained\n",
    "pca = PCA()\n",
    "pca.fit(df.T.values)\n",
    "n_features = df.shape[1]\n",
    "print(n_features)\n",
    "print(len(pca.explained_variance_ratio_.cumsum()))\n",
    "plt.plot(range(1,n_features-1),pca.explained_variance_ratio_.cumsum())\n",
    "plt.xlim(0,50);\n",
    "features = df.columns\n",
    "plt.xticks()\n",
    "plt.xlabel('Number of principal components')\n",
    "plt.ylabel('Fraction of variance explained');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THe transformed values, ordered from highest to lowest variance dimensions\n",
    "transformed = pca.transform(df.values.T)\n",
    "\n",
    "\n",
    "#model_idx = [idx for idx in enumerate(df.index.values) if type(idx)==str]\n",
    "'''\n",
    "#label_model_no_trans_df\n",
    "model_no_trans_df.index.name = 'Cell_ID'\n",
    "model_df = model_no_trans_df.copy()\n",
    "model_df.index.name = 'Cell_ID'\n",
    "'''\n",
    "# make experiment dataframe\n",
    "experiment_idx = [idx for idx in df.index.values if type(idx)==int]\n",
    "model_no_trans_df = df[~df.index.isin(experiment_idx)]\n",
    "experiment_idx_labels = [(i,idx) for i,idx in enumerate(df.index.values) if type(idx)==int]\n",
    "\n",
    "#model_df\n",
    "#df.labels\n",
    "model_no_trans_df\n",
    "experiment_idx_labels = [i[0] for i in experiment_idx_labels]\n",
    "experiment_idx_labels\n",
    "model_no_trans_df\n",
    "model_index_labels = ~df.index.isin(experiment_idx)\n",
    "\n",
    "model_index_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do an isomap embedding in 2 dimensions\n",
    "isomap = Isomap(n_components=2)\n",
    "isomap.fit(df.values)\n",
    "iso = isomap.embedding_.T\n",
    "# Plot that isomap embedding. Each is a model (or a cell, for data)\n",
    "#plt.scatter(iso);\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.scatter(iso[0,experiment_idx_labels],iso[1,experiment_idx_labels],c='blue',cmap='rainbow',label='data')\n",
    "plt.scatter(iso[0,model_index_labels],iso[1,model_index_labels],c='red',cmap='rainbow',label='models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = 1.5\n",
    "strong = 3.0\n",
    "easy_map = [\n",
    "            {'AP12AmplitudeDropTest':standard},\n",
    "            {'AP1SSAmplitudeChangeTest':standard},\n",
    "            {'AP1AmplitudeTest':standard},\n",
    "            {'AP1WidthHalfHeightTest':standard},\n",
    "            {'AP1WidthPeakToTroughTest':standard},\n",
    "            {'AP1RateOfChangePeakToTroughTest':standard},\n",
    "            {'AP1AHPDepthTest':standard},\n",
    "            {'AP2AmplitudeTest':standard},\n",
    "            {'AP2WidthHalfHeightTest':standard},\n",
    "            {'AP2WidthPeakToTroughTest':standard},\n",
    "            {'AP2RateOfChangePeakToTroughTest':standard},\n",
    "            {'AP2AHPDepthTest':standard},\n",
    "            {'AP12AmplitudeChangePercentTest':standard},\n",
    "            {'AP12HalfWidthChangePercentTest':standard},\n",
    "            {'AP12RateOfChangePeakToTroughPercentChangeTest':standard},\n",
    "            {'AP12AHPDepthPercentChangeTest':standard},\n",
    "            {'InputResistanceTest':str('ir_currents')},\n",
    "            {'AP1DelayMeanTest':standard},\n",
    "            {'AP1DelaySDTest':standard},\n",
    "            {'AP2DelayMeanTest':standard},\n",
    "            {'AP2DelaySDTest':standard},\n",
    "            {'Burst1ISIMeanTest':standard},\n",
    "            {'Burst1ISISDTest':standard},\n",
    "            {'InitialAccommodationMeanTest':standard},\n",
    "            {'SSAccommodationMeanTest':standard},\n",
    "            {'AccommodationRateToSSTest':standard},\n",
    "            {'AccommodationAtSSMeanTest':standard},\n",
    "            {'AccommodationRateMeanAtSSTest':standard},\n",
    "            {'ISICVTest':standard},\n",
    "            {'ISIMedianTest':standard},\n",
    "            {'ISIBurstMeanChangeTest':standard},\n",
    "            {'SpikeRateStrongStimTest':strong},\n",
    "            {'AP1DelayMeanStrongStimTest':strong},\n",
    "            {'AP1DelaySDStrongStimTest':strong},\n",
    "            {'AP2DelayMeanStrongStimTest':strong},\n",
    "            {'AP2DelaySDStrongStimTest':strong},\n",
    "            {'Burst1ISIMeanStrongStimTest':strong},\n",
    "            {'Burst1ISISDStrongStimTest':strong},\n",
    "        ]\n",
    "\n",
    "dm_labels = [list(keys.keys())[0]+str('_')+str(list(keys.values())[0])+str('x') for keys in easy_map ]\n",
    "#dm_labels['AHP_depth_abs_slow_1.5x']\n",
    "\n",
    "#dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "druck2.columns        \n",
    "#df.append(druck2)\n",
    "druck2\n",
    "#df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "druck2.columns\n",
    "df.append(druck2)\n",
    "df['AP12AmplitudeDropTest_1.5x']\n",
    "druck2['AP12AmplitudeDropTest_1.5x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AP12AmplitudeDropTest_1.5x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
