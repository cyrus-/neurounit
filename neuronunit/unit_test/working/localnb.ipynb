{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot([0,1],[1,0])\n",
    "from neuronunit.optimisation import make_sim_tests\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "from neuronunit.plottools import plot_score_history1\n",
    "from neuronunit.optimisation.optimization_management import check_binary_match\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Data \n",
    "and select model parameters that are free to vary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fps = ['c','a','b']\n",
    "model_type=\"RAW\"\n",
    "sim_tests, OM, target = make_sim_tests.test_all_objective_test(fps,model_type=model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MU = 10 \n",
    "NGEN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GA Generation Progress:  35%|███▍      | 17/49 [08:15<15:37, 29.30s/it]"
     ]
    }
   ],
   "source": [
    "results = sim_tests.optimize(backend=model_type,\\\n",
    "        protocol={'allen': False, 'elephant': True},\\\n",
    "            MU=MU,NGEN=NGEN,plot=True,free_parameters=fps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = results['pf'][0].dtc\n",
    "hof = results['hof'][0].dtc\n",
    "OM.tests = opt.tests\n",
    "opt = OM.format_test(opt)\n",
    "opt.self_evaluate()\n",
    "opt = OM.get_agreement(opt)\n",
    "display(opt.agreement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should be more succint than above.\n",
    "agreement = opt.get_agreement().agreement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at evolution History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = plot_score_history1(results)\n",
    "from neuronunit.optimisation.optimization_management import check_binary_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = target.dtc_to_model()\n",
    "check_binary_match(opt,target,snippets=True)\n",
    "target = OM.format_test(target)\n",
    "simulated_data_tests = target.tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_binary_match(opt,target,snippets=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    opt.attrs.pop('dt',None)\n",
    "    opt.attrs.pop('Iext',None)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame([opt.attrs]))\n",
    "import copy\n",
    "temp = {}\n",
    "for k in opt.attrs.keys():\n",
    "    temp[k] = target.attrs[k]\n",
    "display(pd.DataFrame([temp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame([{k.name:v for k,v in opt.SA.items()}]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What where the values of model parameters that where held constant?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = opt.dtc_to_model().default_attrs\n",
    "df1 = target.dtc_to_model().default_attrs\n",
    "hc = {}\n",
    "\n",
    "try:\n",
    "    df0.attrs.pop('dt',None)\n",
    "    df0.attrs.pop('Iext',None)\n",
    "    opt.attrs.pop('dt',None)\n",
    "    opt.attrs.pop('Iext',None)\n",
    "\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for k,v in df0.items():\n",
    "    if k not in opt.attrs.keys():\n",
    "        assert df0[k] == df1[k]\n",
    "        hc[k] = v        \n",
    "display(\"Held constant:\")\n",
    "display(pd.DataFrame([hc]))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the Pareto Front encircles the best solution without sampling directly on top of it.\n",
    "Does piercing the center get us closer to the hall of fame?\n",
    "Below, plot HOF[0]/PF[0] are they the same model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_binary_match(opt,hof,snippets=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the neighbourhood of \n",
    "the Optimal solution is now syntatically easy\n",
    "\n",
    "## Make ranges to explore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuronunit.optimisation.model_parameters import MODEL_PARAMS\n",
    "a_range = MODEL_PARAMS[\"RAW\"]['a']\n",
    "grid_a = np.linspace(a_range[0],a_range[1],10)\n",
    "b_range = MODEL_PARAMS[\"RAW\"]['b']\n",
    "grid_b = np.linspace(b_range[0],b_range[1],10);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutate a parameter in a dimension of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_sum = np.sum(opt.SA.values)\n",
    "for_scatter = (opt.attrs['a'],opt_sum)\n",
    "from tqdm import tqdm\n",
    "collect = []\n",
    "mutant = copy.copy(opt)\n",
    "for a in tqdm(grid_a):\n",
    "    # non random mutation\n",
    "    mutant.attrs['a'] = a\n",
    "    # Evaluate NU test suite\n",
    "    mutant.self_evaluate()\n",
    "    # sum components (optional)\n",
    "    fit = np.sum(mutant.SA.values)\n",
    "    collect.append(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(grid_a,collect)\n",
    "plt.scatter(for_scatter[0],for_scatter[1],label='optima')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot seemed to have two steep wells, of low error about the optima.\n",
    "\n",
    "It might not be reasonable to expect to sample every such well, as the stochastic nature of the GA is not garunteed to sample small and and focused pockets of error change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_sum = np.sum(opt.SA.values)\n",
    "for_scatter = (opt.attrs['b'],opt_sum)\n",
    "from tqdm import tqdm\n",
    "collect = []\n",
    "mutant = copy.copy(opt)\n",
    "for b in tqdm(grid_b):\n",
    "    # non random mutation\n",
    "    mutant.attrs['b'] = b\n",
    "    # Evaluate NU test suite\n",
    "    mutant.self_evaluate()\n",
    "    # sum components (optional)\n",
    "    fit = np.sum(mutant.SA.values)\n",
    "    collect.append(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(grid_b,collect)\n",
    "for_scatter = (opt.attrs['b'],opt_sum)\n",
    "plt.scatter(for_scatter[0],for_scatter[1],label='optima')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
