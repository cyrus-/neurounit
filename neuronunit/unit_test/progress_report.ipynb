{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Report and Documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Rick's comments:\n",
    "*4)* Label your axes and title your plots (e.g. cell 17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I have been working on. \n",
    "1. Updating Docker-stacks dockerfile build instructions in order to make a HPC ready Dockerbuild.\n",
    "2. Updating the aforementioned stack, in order to perform regular maintance and to fix build problems due to \n",
    "updates of upstream software sources.\n",
    "3. Debugging and visualisation of the BluePyOpt GA algorithm.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other work\n",
    "1.  The people at BluePyOpt (Werner) agreed to make a scidash branch, they want me to make use extensibility and inheritence. Such that my scidash derivation of BluePyOpt inherits from and extends as much as possible the parent class of their elitist branch at BPO.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolving Docker Build issues. \n",
    "\n",
    "* Issue 1 Upgrading pip to pip10, breaks NU installation.\n",
    "Work around: re-write setup.py to exclude problems associated with process dependency links.\n",
    "\n",
    "    \n",
    "* Issue 2   \n",
    "Developing in BPO and NU simultaneously requires a dockerfile at to build from a location at one location down in the directory hierarchy: There is a need to install both packages using the pip -e paradigm (file changes during session at developer locations, effect modules that are in sys.path):\n",
    "** ADD neuronunit neuronunit\n",
    "** ADD BluePyOpt BluePyOpt\n",
    "To this end scidash opt was created.\n",
    "https://github.com/russelljjarvis/scidashopt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "I have recently created a comparison between the GA, and the optimizer, on a very simple 2 parameter 10 point optimization problem. This small toy problem should confirm whether the optimizer is able to find the same solution as the grid search.\n",
    "\n",
    "# 2D problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c3a87ffa71ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'grid_cell_results.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "class WSListIndividual(list):\n",
    "    \"\"\"Individual consisting of list with weighted sum field\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        self.rheobase = None\n",
    "        super(WSListIndividual, self).__init__(*args, **kwargs)\n",
    "\n",
    "\n",
    "with open('grid_cell_results.p','rb') as f:\n",
    "    results = pickle.load(f)\n",
    "    \n",
    "\n",
    "with open('all_ga_cell.p','rb') as f:\n",
    "    package = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "garanked = [ (r.dtc.attrs , sum(r.dtc.scores.values())) for r in package[0] ]\n",
    "garanked = sorted(garanked, key=lambda w: w[1]) \n",
    "miniga = garanked[0][1]\n",
    "maxiga = garanked[-1][1]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results[0]\n",
    "gridranked = [ (r.dtc.attrs , sum(r.dtc.scores.values())) for r in results[0] ]\n",
    "gridranked = sorted(ranked, key=lambda w: w[1]) \n",
    "mini = gridranked[0][1]\n",
    "maxi = gridranked[-1][1]\n",
    "\n",
    "\n",
    "print(mini,maxi)\n",
    "quantize_distance = list(np.linspace(mini,maxi,10))\n",
    "\n",
    "# check that the nsga error is in the bottom 1/5th of the entire error range.\n",
    "print('Report: ')\n",
    "print(\"Success\" if bool(miniga < quantize_distance[0]) else \"Failure\")\n",
    "print(\"The nsga error %f is in the bottom 1/5th of the entire error range\" % miniga)\n",
    "print(\"Minimum = %f; 20th percentile = %f; Maximum = %f\" % (mini,quantize_distance[0],maxi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "with open('all_ga_cell.p','rb') as f:\n",
    "   results = pickle.load(f)\n",
    "print(results[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig, axes = plt.subplots(figsize=(10, 10), facecolor='white')\n",
    "log = results[3]\n",
    "gen_numbers = [ i for i in range(0,len(log.select('gen'))) ]\n",
    "print(gen_numbers)\n",
    "\n",
    "hof = val['hof_py']\n",
    "mean = np.array([ np.sum(i) for i in log.select('avg')])\n",
    "std = np.array([ np.sqrt(np.mean(np.square(i))) for i in log.select('std')])\n",
    "minimum = np.array([ np.sqrt(np.mean(np.square(i))) for i in log.select('min')])\n",
    "\n",
    "stdminus = mean - std\n",
    "stdplus = mean + std\n",
    "\n",
    "axes.plot(\n",
    "    gen_numbers,\n",
    "    mean,\n",
    "    color='black',\n",
    "    linewidth=2,\n",
    "    label='population average')\n",
    "\n",
    "axes.plot(\n",
    "    gen_numbers,\n",
    "    minimum,\n",
    "    color='black',\n",
    "    linewidth=2,\n",
    "    label='population minimum')\n",
    "axes.fill_between(gen_numbers, stdminus, stdplus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What works \n",
    "\n",
    "Plotting of candidate solutions in 2D error surface slices, and using this as a means to validate GA candidate solutions, see below.\n",
    "\n",
    "Presenting tabular data as pandas Data frames. see below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does not yet work\n",
    "The mean errors of the Genetic Algorithm have a downward trend, but they do not reliably decrease like the gradient descent alogorithm.\n",
    "in order to circumvent this percieved flaw, two interventions have been used.\n",
    "\n",
    "\n",
    "https://github.com/russelljjarvis/neuronunit/blob/dev/neuronunit/unit_test/pipe_entry_point.py#L86\n",
    "\n",
    "lines 152-154 have \n",
    "https://github.com/russelljjarvis/BluePyOpt/blob/elitism/bluepyopt/deapext/algorithms.py#L155-156\n",
    "Line 152-154 of BluePyOpt/bluepyopt/deapext/algorithms.py\n",
    "\n",
    "have been put in, to enforce that genes that have been selected to be parents for breeding are fitter than the on average than the rest of the population of genes. I believe that the usage of this particular assert clause, will help to further isolate the cause of instabliity in fitness evolution.\n",
    "\n",
    "Furthermore, I have created methods for plotting the Hall of Fame and it's evolution as a means to corroborate GA results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This motivated me to stop convenient, but lazy reading and writing of attributes from disk, which could otherwise be stored\n",
    "in the Data Transport Container. \n",
    "* I updated the file models/backends/neuron.py in two crucial places to stop unnecessary file reads.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will work with more time\n",
    "\n",
    "Grid Search on HPC over docker seems to mostly work.\n",
    "\n",
    "A docker container was made especially to run on a Spike server administratored by Renate. To this end, I have created a dedicated [branch](https://github.com/russelljjarvis/docker-stacks-returned/tree/hpc), with a different entrypoint. \n",
    "The docker container builds The container has an entrypoint which runs an exhaustive search simulation. \n",
    "\n",
    "\n",
    "## The command: \n",
    "```\n",
    "nohup docker run -v /home/rjarvis/git/neuronunit:/home/jovyan/neuronunit scidash/neuronunit-optimization```\n",
    "is sufficient to run the file\n",
    "```/home/rjarvis/git/neuronunit/neuronunit/unit_test/grid_entry_point.py``` with python.\n",
    "\n",
    "On the neurospike server many core are available ~50 I have used 36 in dask bags function \n",
    "`db.from_sequence(items,npartitions = 36)`. This occurs in two places in the exhaustive (grid) search algorithm. In the find rheobase algorithm, and the neuronounit score algorithm. \n",
    "\n",
    "Once the NU algorithm has run, files from the spike server are aggregated in a directory accessible to this script, using the command:\n",
    "\n",
    "```alias cnspike='cd /Users/rjjarvis/Dropbox\\ \\(ASU\\)/capsule/neuronunit/neuronunit/unit_test/grid_server; scp -P 2200 rjarvis@129.219.30.18:/home/rjarvis/git/neuronunit/neuronunit/unit_test/*.p .'```\n",
    "\n",
    "\n",
    "Previously when running this file, it ran without error, however, because I forgot to mount a volume, pickle files generated in this session where not stored. I have since executed the command using a mounted volume, and I am now waiting for an output from the command.\n",
    "\n",
    "It is expected that this will write four different files, with the prefix=grid_cell, and suffix=neurolex ID\n",
    "At the location neuronunit/neuronunit/unit_test on the spike server.\n",
    "\n",
    "\n",
    "This file runs without error\n",
    "As confirmed by running \n",
    "```tail $HOME/nohup.out```, on the spike server, however the expected pickle files are not present there.\n",
    "\n",
    "I expect this is because on the first run, I failed to define a mount point, to mount a volume in the docker run command, such that results would write to a location on the HOST OS upon exiting. \n",
    "\n",
    "\n",
    "I am currently waiting for the output of this job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('dump_all_cells','rb') as f:\n",
    "   pipe_results = pickle.load(f)\n",
    "\n",
    "len(pipe_results['100201']['gen_vs_hof'])\n",
    "pipe_results['100201']['gen_vs_hof'][-3].fitness.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Assumptions about this code:\n",
    "The NB was launched with a command that mounts two volumes inside a docker container. \n",
    "In the future invocation of this script will be simplified greatly. NU is from a specific fork and branch -b results https://github.com/russelljjarvis/neuronunit \n",
    "BluePyOpt is also from a specific fork and branch: -b elitism https://github.com/russelljjarvis/BluePyOpt\n",
    "\n",
    "Below BASH code for Ubuntu host:\n",
    "\n",
    "``` bash\n",
    "cd ~/git/neuronunit; sudo docker run -it -v `pwd`:/home/jovyan/neuronunit -v ~/git/BluePyOpt:/home/jovyan/BluePyOpt neuronunit-optimization /bin/bash'\n",
    "```\n",
    "\n",
    "## Parallel Environment.\n",
    "Parallelisation module: dask distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('jupyter trust test_ga_versus_grid.ipynb'); #suppress the untrusted notebook warning.\n",
    "import deap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cell_names = list(pipe_results.keys())\n",
    "param_names = list(pipe_results[list(pipe_results.keys())[0]]['pop'][0].dtc.attrs.keys())\n",
    "df = pd.DataFrame(index=pipe_results.keys(),columns=param_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for index, val in enumerate(pipe_results.values()):\n",
    "    if index == 0:\n",
    "        sci = pd.DataFrame(list(val['pop'][0].dtc.scores.values())).T\n",
    "    else:    \n",
    "        sci = sci.append(pd.DataFrame(list(val['pop'][0].dtc.scores.values())).T)\n",
    "        \n",
    "sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for index, val in enumerate(pipe_results.values()):\n",
    "    if index == 0:\n",
    "        attrs = pd.DataFrame(list(val['pop'][0].dtc.attrs.values())).T\n",
    "    else:    \n",
    "        attrs = attrs.append(pd.DataFrame(list(val['pop'][0].dtc.attrs.values())).T)\n",
    "        \n",
    "attrs.columns = val['pop'][0].dtc.attrs.keys()    \n",
    "#print(attrs)\n",
    "attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "for index, val in enumerate(pipe_results.values()):\n",
    "    if index == 0:\n",
    "        #,columns=['Dice number','value'],index=[1,2,3,4])\n",
    "        rheobase = pd.DataFrame([i.dtc.rheobase for i in val['pop']]).T\n",
    "    else:    \n",
    "        rheobase = rheobase.append(pd.DataFrame([i.dtc.rheobase for i in val['pop']]).T)\n",
    "        \n",
    "rheobase\n",
    "\n",
    "names = [ str('generation: ')+str(i) for i in range(0,len(rheobase)) ]\n",
    "\n",
    "rheobase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################\n",
    "# GA parameters:\n",
    "about $10^{3}=30$ models will be made, excluding rheobase search.\n",
    "################\n",
    "\n",
    "\n",
    "# Choice of selection criteria is important. \n",
    "Here we use BluepyOpts IBEA, such that it can be compared to NSGA2.\n",
    "\n",
    "https://link.springer.com/article/10.1007/s00500-005-0027-5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MU = 6; NGEN = 6; CXPB = 0.9\n",
    "USE_CACHED_GA = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "################\n",
    "# Grid search parameters:\n",
    "$ 2^{10}=1024 $ models, will be made excluding rheobase search\n",
    "################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An oppurtunity to improve grid search, by increasing resolution of search intervals given a first pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuronunit.plottools import plot_surface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below two error surface slices from the hypervolume are plotted.\n",
    "The data that is plotted consists of the error as experienced by the GA.\n",
    "Note: the GA performs an incomplete, and efficient sampling of the parameter space, and therefore sample points are irregularly spaced. Polygon interpolation is used to visualize error gradients. Existing plotting code from the package BluePyOpt has been extended for this purpose.\n",
    "Light blue dots indicate local minima's of error experienced by the NSGA algrorithm.\n",
    "\n",
    "\n",
    "Mostly these plots show that a low error solution was found in each 2D plane, however occasionally the plots show, that a non optimal solution was arrived at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, val in enumerate(pipe_results.values()):\n",
    "    td = val['td_py']\n",
    "    history = val['history']\n",
    "\n",
    "    plot_surface('a','b',td,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for index, val in enumerate(pipe_results.values()):\n",
    "    td = val['td_py']\n",
    "    history = val['history']\n",
    "\n",
    "    plot_surface('v0','vt',td,history)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(pipe_results.keys())\n",
    "for k in pipe_results.keys():\n",
    "    print(pipe_results[k]['hranked'])\n",
    "pipe_results['100201']['hranked']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am currently writing code that should enable the plotting of HOF values versus generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dtcs = list(filter(lambda d: hasattr(d,'dtc'), pipe_results['100201']['hranked']))\n",
    "dtcs = [d.dtc for d in dtcs ]\n",
    "fitness = list(filter(lambda f: hasattr(f,'fitness'), pipe_results['100201']['hranked']))\n",
    "fit_v_gen = [np.sum(f.fitness.values) for f in fitness ]\n",
    "\n",
    "fit_v_gen\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(pipe_results.keys())\n",
    "\n",
    "scores = list(filter(lambda dtc: hasattr(dtc,'score'), dtcs))\n",
    "\n",
    "print(scores)\n",
    "import pickle\n",
    "dominate\n",
    "with open('protected/dominated_by_rheobase.p','rb') as f:\n",
    "   pipe_results = pickle.load(f)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "fig, axes = plt.subplots(figsize=(10, 10), facecolor='white')\n",
    "val = list(pipe_results.values())[0]\n",
    "log = val['log']\n",
    "gen_numbers = [ i for i in range(0,len(log.select('gen'))) ]\n",
    "print(gen_numbers)\n",
    "\n",
    "\n",
    "hof = val['hof_py']\n",
    "mean = np.array([i for i in log.select('avg')])\n",
    "std = np.array([ i for i in log.select('std')])\n",
    "minimum = np.array([ i for i in log.select('min')])\n",
    "best_line = [None,None]\n",
    "best_line +=val['hranked']\n",
    "\n",
    "\n",
    "stdminus = mean - std\n",
    "stdplus = mean + std\n",
    "\n",
    "axes.plot(\n",
    "    gen_numbers,\n",
    "    mean,\n",
    "    color='black',\n",
    "    linewidth=2,\n",
    "    label='population average')\n",
    "\n",
    "\n",
    "axes.plot(\n",
    "    gen_numbers,\n",
    "    best_line,\n",
    "    color='black',\n",
    "    linewidth=2,\n",
    "    label='population average')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "axes.plot(\n",
    "    gen_numbers,\n",
    "    minimum,\n",
    "    color='black',\n",
    "    linewidth=2,\n",
    "    label='population minimum')\n",
    "axes.fill_between(gen_numbers, stdminus, stdplus)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason the global minimum solution is not converged on, as shown by the evolution of errors below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "best_line = None\n",
    " \n",
    "for k in pipe_results.keys():\n",
    "    fig, axes = plt.subplots(figsize=(10, 10), facecolor='white')\n",
    "\n",
    "    historiesc = [list(h.dtc.scores.values()) for h in pipe_results[k]['history'].genealogy_history.values() ]\n",
    "\n",
    "    historiest = [(sum(h.dtc.scores.values()),h.dtc) for h in pipe_results[k]['history'].genealogy_history.values() ]\n",
    "    ranked = sorted(historiest, key=lambda w: w[0],reverse = True) \n",
    "    pipe_results[k]['abs_min'] = ranked[0][1]\n",
    "    \n",
    "    historiess = [sum(h.dtc.scores.values()) for h in pipe_results[k]['history'].genealogy_history.values() ]\n",
    "    min_line = np.min(historiess)    \n",
    "    axes.plot([i for i in range(0,len(historiess)) ],\n",
    "        historiess,\n",
    "        color='black',\n",
    "        linewidth=2,\n",
    "        label='population average')\n",
    "    \n",
    "\n",
    "    axes.plot(\n",
    "        [i for i in range(0, len(historiess)) ],\n",
    "        [min_line for i in range(0, len(historiess)) ],\n",
    "        color='black',\n",
    "        linewidth=2,\n",
    "        label='population average')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, the GA population does not converge to the absolute minimum, although it does sample it.\n",
    "Perhaps the absolute minimum is a highly dominated solution, which is a testable hypthosis.\n",
    "\n",
    "None the less because the GA samples the absolute minimum, this value can be corroborated with the GA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "print(pipe_results[k]['abs_min'].scores)\n",
    "print(sum(pipe_results[k]['abs_min'].scores.values()))\n",
    "\n",
    "print(pipe_results[k]['abs_min'].attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b7f0102aa470>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ggplot'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hof_py'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.style.use('ggplot')\n",
    "fig, axes = plt.subplots(figsize=(10, 10), facecolor='white')\n",
    "val = list(pipe_results.values())[1]\n",
    "log = val['log']\n",
    "hof = val['hof_py']\n",
    "mean = np.array([ np.sum(i) for i in log.select('avg')])\n",
    "std = np.array([ np.sum(i) for i in log.select('std')])\n",
    "gen_numbers = [ i for i in range(0,len(log.select('gen'))) ]\n",
    "\n",
    "\n",
    "print(len(mean),len(std))\n",
    "minimum = np.array([ np.sqrt(np.mean(np.square(i))) for i in log.select('min')])\n",
    "\n",
    "historiess = [sum(h.dtc.scores.values()) for h in pipe_results[k]['history'].genealogy_history.values() ]\n",
    "min_value = np.min(historiess)    \n",
    "\n",
    "\n",
    "stdminus = mean - std\n",
    "stdplus = mean + std\n",
    "print\n",
    "\n",
    "\n",
    "axes.plot(\n",
    "    gen_numbers,\n",
    "    mean,\n",
    "    color='black',\n",
    "    linewidth=2,\n",
    "    label='population mean')\n",
    "\n",
    "\n",
    "\n",
    "axes.fill_between(gen_numbers, stdminus, stdplus,label='+- std deviation')\n",
    "plt.xlabel('generation')\n",
    "plt.ylabel('error')\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New insights:\n",
    "My method for calculating population mean was bad.\n",
    "## My use of BluePyOpt was bad, in that it used ne elite =0 \n",
    "I suspect that grid search is going to find dominated optima. Therefore I should use an optimisation technique that also favors global optima.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_line = None\n",
    "for k in pipe_results.keys():\n",
    "    plt.style.use('ggplot')\n",
    "    fig, axes = plt.subplots(figsize=(10, 10), facecolor='white')\n",
    "    val = pipe_results[k]\n",
    "    log = val['log']\n",
    "    gen_numbers = [ i for i in range(0,len(log.select('gen'))) ]\n",
    "    historiess = [sum(h.dtc.scores.values()) for h in pipe_results[k]['history'].genealogy_history.values() ]\n",
    "    min_value = np.min(historiess)  \n",
    "    \n",
    "\n",
    "    axes.plot([i for i in range(0,len(pipe_results[k]['componentsh'] )) ],\n",
    "        pipe_results[k]['componentsh'] ,\n",
    "        linewidth=2,\n",
    "        label='population average')\n",
    "    \n",
    "\n",
    "    axes.plot(\n",
    "        [i for i in range(0,len(pipe_results[k]['componentsh'] )) ],\n",
    "        [min_value for i in range(0,len(pipe_results[k]['componentsh'] ))  ],\n",
    "        color='black',\n",
    "        linewidth=2,\n",
    "        label='population average')\n",
    "    \n",
    "    for i in pipe_results[k]['componentsh']:\n",
    "        print(sum(i),min_value)\n",
    "        if sum(i) == min_value:\n",
    "            print('yes')\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Rational, want to find out how dominated the best solution is:\n",
    "##\n",
    "best_line = None\n",
    "for k in pipe_results.keys():\n",
    "    plt.style.use('ggplot')\n",
    "    fig, axes = plt.subplots(figsize=(10, 10), facecolor='white')\n",
    "    val = pipe_results[k]\n",
    "    log = val['log']\n",
    "    \n",
    "    historiest = [(sum(h.dtc.scores.values()),h.dtc) for h in pipe_results[k]['history'].genealogy_history.values() ]\n",
    "    ranked = sorted(historiest, key=lambda w: w[0],reverse = True) \n",
    "    pipe_results[k]['abs_min'] = ranked[0][1]\n",
    "    min_value = ranked[0][0]\n",
    "    \n",
    "    axis = [ i for i in range(0,len(ranked[0][1].scores.values())) ]\n",
    "    plt.bar(axis,list(ranked[0][1].scores.values()),tick_label=list(ranked[0][1].scores.keys()))\n",
    "    fig.autofmt_xdate()\n",
    "    plt.title('neuroelectro_cell_{0}_{1}'.format(str(k),str('solution components')))\n",
    "    plt.legend('left')\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for index, val in enumerate(pipe_results.values()):\n",
    "\n",
    "    print(val['gen_vs_hof'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment on plot\n",
    "The plot shows the mean error value of the population as the GA evolves it's population. The red interval at any instant is the standard deviation of the error. The fact that the mean GA error is able to have a net upwards trajectory, after experiencing a temporary downwards trajectory, demonstrates that the GA retains a drive to explore, and is resiliant against being stuck in a local minima. Also in the above plot population variance in error stays remarkably constant, in this way BluePyOpts selection criteria SELIBEA contrasts with DEAPs native selection strategy NSGA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment on plot\n",
    "There is good agreement between traces produced by the best candidate found by Genetic Algorithm, and exhaustive grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize distance between minimimum error and maximum error.\n",
    "This step will allow the GA's performance to be located within or below the range of error found by grid search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(maxi)\n",
    "print(mini)\n",
    "print(miniga)\n",
    "quantize_distance = list(np.linspace(mini,maxi,10))\n",
    "\n",
    "# check that the nsga error is in the bottom 1/5th of the entire error range.\n",
    "print('Report: ')\n",
    "print(\"Success\" if bool(miniga < quantize_distance[0]) else \"Failure\")\n",
    "print(\"The nsga error %f is in the bottom 1/5th of the entire error range\" % miniga)\n",
    "print(\"Minimum = %f; 20th percentile = %f; Maximum = %f\" % (mini,quantize_distance[0],maxi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below reports on the differences between between attributes of best models found via grid versus attributes of best models found via GA search:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from neuronunit.optimization import evaluate_as_module as eam\n",
    "NSGAO = NSGA(0.85)\n",
    "NSGAO.setnparams(nparams=nparams,provided_keys=provided_keys)\n",
    "#td = eam.get_trans_dict(NSGAO.subset)\n",
    "#print(td)\n",
    "td = { v:k for k,v in enumerate(td) }\n",
    "from neuronunit.optimization import model_parameters as modelp\n",
    "mp = modelp.model_params\n",
    "#minimaga = pareto_dtc[0]\n",
    "for k,v in minimagr_dtc.attrs.items():\n",
    "    #hvgrid = np.linspace(np.min(mp[k]),np.max(mp[k]),10)\n",
    "    dimension_length = np.max(mp[k]) - np.min(mp[k])\n",
    "    solution_distance_in_1D = np.abs(float(hof[0][td[k]]))-np.abs(float(v))\n",
    "        \n",
    "    #solution_distance_in_1D = np.abs(float(minimaga.attrs[k]))-np.abs(float(v))\n",
    "    relative_distance = dimension_length/solution_distance_in_1D\n",
    "    print('the difference between brute force candidates model parameters and the GA\\'s model parameters:')\n",
    "    print(float(hof[0][td[k]])-float(v),hof[0][td[k]],v,k)\n",
    "    print('the relative distance scaled by the length of the parameter dimension of interest:')\n",
    "    print(relative_distance)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print('the difference between the bf error and the GA\\'s error:')\n",
    "print('grid search:')\n",
    "from numpy import square, mean, sqrt\n",
    "rmsg = sqrt(mean(square(list(minimagr_dtc.scores.values()))))\n",
    "print(rmsg)\n",
    "print('ga:')\n",
    "rmsga = sqrt(mean(square(list(dtc_pop[0].scores.values()))))\n",
    "print(rmsga)\n",
    "print('Hall of Fame front')\n",
    "print(sqrt(mean(square(list(hof[0].fitness.values)))))\n",
    "print(miniga)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If any time is left over, may as well compute a more accurate grid, to better quantify GA performance in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from neuronunit.optimization import get_neab\n",
    "#fi_basket = {'nlex_id':'NLXCELL:100201'}\n",
    "neuron = {'nlex_id': 'nifext_50'} \n",
    "\n",
    "error_criterion, inh_observations = get_neab.get_neuron_criteria(fi_basket)\n",
    "print(error_criterion)\n",
    "\n",
    "from bluepyopt.deapext.optimisations import DEAPOptimisation\n",
    "\n",
    "DO = DEAPOptimisation(error_criterion=error_criterion)\n",
    "DO.setnparams(nparams = nparams, provided_keys = provided_keys)\n",
    "pop, hof, log, history, td, gen_vs_hof = DO.run(offspring_size = MU, max_ngen = NGEN, cp_frequency=4,cp_filename='checkpointedGA.p')\n",
    "with open('ga_dump.p','wb') as f:\n",
    "   pickle.dump([pop, log, history, hof, td],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer V pyramidal cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
