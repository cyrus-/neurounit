{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Rheobase cached data value for from AIBS dataset 354190013\n",
      "attempting to recover from pickled file\n",
      "/home/jovyan/mnt/neuronunit/neuronunit/tests/__init__.py\n",
      "Ignoring included LEMS file: Cells.xml\n",
      "Ignoring included LEMS file: Networks.xml\n",
      "Ignoring included LEMS file: Simulation.xml\n",
      "Mechanisms already loaded from path: /home/jovyan/mnt/neuronunit/neuronunit/tests/NeuroML2.  Aborting.\n",
      "Ignoring included LEMS file: Cells.xml\n",
      "Ignoring included LEMS file: Networks.xml\n",
      "Ignoring included LEMS file: Simulation.xml\n"
     ]
    }
   ],
   "source": [
    "import sys                                                                                                                                                                                                              \n",
    "sys.path[0]='/home/jovyan/mnt/neuronunit'                                                                                                                                                                                                     \n",
    "from neuronunit.tests import get_neab                                                                                                                                                                                                         \n",
    "import neuronunit                                                                                                                                                                                                                             \n",
    "print(neuronunit.tests.__file__)                                                                                                                                                                                                              \n",
    "from neuronunit.tests import utilities as outils                                                                                                                                                                                              \n",
    "                                                                                                                                                                                                                                              \n",
    "from neuronunit.tests import model_parameters as modelp                                                                                                                                                                                       \n",
    "import numpy as np                                                                                                                                                                                                                            \n",
    "model = outils.model                \n",
    "from sciunit import scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'neuronunit.tests.model_parameters' from '/home/jovyan/mnt/neuronunit/neuronunit/tests/model_parameters.py'>\n",
      "0 -73.400262041\n",
      "1 36.4524842237\n",
      "2 -58.5434523695\n",
      "3 0.124862070926\n",
      "4 -4.40578546097e-09\n",
      "5 0.179072394484\n",
      "6 9.72425551651e-05\n",
      "7 0.0007796795291\n",
      "8 -44.6032302514\n",
      "9 -55.9630798427\n",
      "0 -58.5242551327\n",
      "1 33.0107157088\n",
      "2 -72.2325673634\n",
      "3 0.466450283841\n",
      "4 -4.98182194127e-09\n",
      "5 0.146182635518\n",
      "6 9.18533525657e-05\n",
      "7 0.000665578583789\n",
      "8 -41.0093438624\n",
      "9 -55.4635958265\n",
      "0 -72.0455794066\n",
      "1 31.9233006624\n",
      "2 -58.0981394854\n",
      "3 0.343723173798\n",
      "4 -3.71598046509e-09\n",
      "5 0.11340322982\n",
      "6 0.000101491907381\n",
      "7 0.000935026972412\n",
      "8 -46.8031386223\n",
      "9 -58.0301612763\n",
      "0 -59.7327082337\n",
      "1 38.8677395605\n",
      "2 -70.6003927936\n",
      "3 0.473705067577\n",
      "4 -4.76178306559e-09\n",
      "5 0.0960889386051\n",
      "6 0.000107717893413\n",
      "7 0.000786184233302\n",
      "8 -42.4686230142\n",
      "9 -56.1717271284\n",
      "0 -70.1297605684\n",
      "1 34.9267334155\n",
      "2 -74.9669130686\n",
      "3 0.841101492027\n",
      "4 -4.3089007837e-09\n",
      "5 0.0930380532936\n",
      "6 0.000103533932099\n",
      "7 0.00125364654496\n",
      "8 -40.2240399134\n",
      "9 -59.0841919219\n",
      "0 -71.403643421\n",
      "1 32.8666938894\n",
      "2 -65.9587983304\n",
      "3 0.205360302886\n",
      "4 -4.98677019763e-09\n",
      "5 0.0621915124302\n",
      "6 9.63191694307e-05\n",
      "7 0.000814449669807\n",
      "8 -30.2259128781\n",
      "9 -55.7153809665\n",
      "0 -61.9957442562\n",
      "1 30.9971500441\n",
      "2 -55.3086489663\n",
      "3 0.637358033213\n",
      "4 -3.59110668305e-09\n",
      "5 0.0626037028273\n",
      "6 0.000102733065054\n",
      "7 0.00112671120764\n",
      "8 -39.6564854033\n",
      "9 -58.1469235273\n",
      "0 -59.1741429525\n",
      "1 31.4293008127\n",
      "2 -54.1140803628\n",
      "3 0.269961798223\n",
      "4 -3.50574332973e-09\n",
      "5 0.0873080393069\n",
      "6 9.52982572612e-05\n",
      "7 0.000714472403151\n",
      "8 -35.5099607236\n",
      "9 -56.8586505774\n",
      "0 -50.8760892162\n",
      "1 30.3209895796\n",
      "2 -62.9710631836\n",
      "3 0.378097010662\n",
      "4 -4.66269473668e-09\n",
      "5 0.108461555257\n",
      "6 9.07665121342e-05\n",
      "7 0.000915524099345\n",
      "8 -35.4090299639\n",
      "9 -57.7186288739\n",
      "0 -70.3008003059\n",
      "1 32.8530498277\n",
      "2 -69.2803459151\n",
      "3 0.462731901443\n",
      "4 -4.93226788434e-09\n",
      "5 0.114622190472\n",
      "6 0.000102402890094\n",
      "7 0.00138925204069\n",
      "8 -46.7110693561\n",
      "9 -57.5002300739\n",
      "0 -55.6753078572\n",
      "1 33.2756466453\n",
      "2 -72.1070105137\n",
      "3 0.56015791535\n",
      "4 -3.71807402746e-09\n",
      "5 0.149940925678\n",
      "6 0.000106483000741\n",
      "7 0.00121882670375\n",
      "8 -38.8765531603\n",
      "9 -59.9165879026\n",
      "0 -52.4296174173\n",
      "1 35.6498654158\n",
      "2 -51.6320725665\n",
      "3 0.695893592852\n",
      "4 -4.69909120504e-09\n",
      "5 0.0958291260207\n",
      "6 9.47853193721e-05\n",
      "7 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/scoop-0.7.2.0-py3.5.egg/scoop/fallbacks.py:46: RuntimeWarning: SCOOP was not started properly.\n",
      "Be sure to start your program with the '-m scoop' parameter. You can find further information in the documentation.\n",
      "Your map call has been replaced by the builtin serial Python map().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000771016165325\n",
      "8 -36.5919387429\n",
      "9 -55.8344549469\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Score:\n",
    "    def __init__(self, score):\n",
    "        self.score = score\n",
    "\n",
    "class Test:\n",
    "    def _optimize(self, model,modelp):\n",
    "        '''\n",
    "        The implementation of optimization, consisting of implementation details.\n",
    "        Inputs a model, and model parameter ranges to expore\n",
    "        Private method for programmer designer.\n",
    "        Outputs the optimal model, its attributes and the low error it resulted in.\n",
    "        '''\n",
    "        from neuronunit.tests import nsga\n",
    "        gap = nsga.GAparams(model)\n",
    "        # The number of generations is 2\n",
    "        gap.NGEN = 3\n",
    "        # The population of genes is 4\n",
    "        gap.MU = 12\n",
    "        \n",
    "        gap.BOUND_LOW = [ np.min(i) for i in modelp.model_params.values() ]\n",
    "        gap.BOUND_UP = [ np.max(i) for i in modelp.model_params.values() ]\n",
    "\n",
    "        vmpop, pop, invalid_ind, pf = nsga.main(gap.NGEN,gap.MU,model,modelp)\n",
    "        some_tuples = (vmpop, pop, invalid_ind, pf)\n",
    "        return pop[0], some_tuples\n",
    "        \n",
    "    def _get_optimization_parameters(self, some_tuples):\n",
    "        vmpop = some_tuples[0]\n",
    "        # Your specific unpacking of tuples that _optimize returns\n",
    "        scores = [ i.score for i in vmpop ]\n",
    "        pop = some_tuples[1]\n",
    "        \n",
    "        errors = pop[0].fitness\n",
    "        parameters = vmpop[0].attrs\n",
    "      \n",
    "        #parameters,errors,scores,_ = zip(*some_tuples)\n",
    "        return parameters,scores\n",
    "\n",
    "    def optimize(self, model, modelp):\n",
    "        '''\n",
    "        The Class users version of optimize\n",
    "        where details are hidden in _optimizae\n",
    "        '''\n",
    "\n",
    "        # Do optimization including repeated calls to judge\n",
    "        models, some_tuples = self._optimize(model,modelp)\n",
    "        parameters, scores = self._get_optimization_parameters(some_tuples)\n",
    "        # Maybe rebuild the original model\n",
    "        # (i.e. restore the true class from the virtual version)\n",
    "\n",
    "        # Your code keeps parameter sets and associated scores\n",
    "        # All the organizing stuff\n",
    "        \n",
    "        \n",
    "        # this a way of looking at solved model parameters, ie candidate solutions from \n",
    "        # the pareto front.\n",
    "\n",
    "        \n",
    "        # Make a ScoreArray (which is basically a pandas dataframe)\n",
    "       \n",
    "        return model, scores, some_tuples\n",
    "\n",
    "    \n",
    "t = Test()\n",
    "\n",
    "\n",
    "model,scores,some_tuples = t.optimize(model,modelp)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sciunit\n",
    "modelp\n",
    "import pandas as pd\n",
    "attributes = [ i.attrs for i in some_tuples[0] ]\n",
    "rheobases = [ i.rheobase for i in some_tuples[0] ]\n",
    "#modelsp = [model.__class__(p) for p in parameters]\n",
    "models = some_tuples[1]\n",
    "sc = pd.DataFrame(scores[0])\n",
    "for j,i in enumerate(models):\n",
    "        i.name = attributes[j]\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pf = some_tuples[3]\n",
    "#modelp.model_params\n",
    "from neuronunit.tests import nsga\n",
    "\n",
    "param_dict = modelp.model_params\n",
    "trans_dict = nsga.get_trans_dict(param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The paretofront stores the best attributes according to the nsga.\n",
    "vms=[]\n",
    "for p in pf:\n",
    "    p = nsga.individual_to_vm(p,trans_dict=trans_dict)\n",
    "    vms.append(p.attrs)\n",
    "    vms.append(p.error)\n",
    "print(vms[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ models[0].name ]\n",
    "model_values0 = pd.DataFrame(data)        \n",
    "model_values0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rheobases[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc1 = pd.DataFrame(scores[1])\n",
    "sc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rheobases[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[ models[1].name ]\n",
    "model_values1 = pd.DataFrame(data)        \n",
    "model_values1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models[1].name        \n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "try:\n",
    "    ground_error = pickle.load(open('big_model_evaulated.pickle','rb'))\n",
    "except:\n",
    "    print('{0} it seems the error truth data does not yet exist, lets create it now '.format(str(False)))\n",
    "    ground_error = list(futures.map(outils.func2map, ground_truth))\n",
    "    pickle.dump(ground_error,open('big_model_evaulated.pickle','wb'))\n",
    "#ground_error_nsga=list(zip(vmpop,pop,invalid_ind))\n",
    "#pickle.dump(ground_error_nsga,open('nsga_evaulated.pickle','wb'))\n",
    "\n",
    "#Get the differences between values obtained via brute force, and those obtained otherwise:\n",
    "sum_errors = [ i[0] for i in ground_error ]\n",
    "composite_errors = [ i[1] for i in ground_error ]\n",
    "attrs = [ i[2] for i in ground_error ]\n",
    "rheobase = [ i[3] for i in ground_error ]\n",
    "\n",
    "indexs = [i for i,j in enumerate(sum_errors) if j==np.min(sum_errors) ][0]\n",
    "indexc = [i for i,j in enumerate(composite_errors) if j==np.min(composite_errors) ][0]\n",
    "#assert indexs == indexc\n",
    "vmpop = some_tuples[0]\n",
    "df_0 = pd.DataFrame([ (k,v,vmpop[0].attrs[k],float(v)-float(vmpop[0].attrs[k])) for k,v in ground_error[indexc][2].items() ])\n",
    "df_1 = pd.DataFrame([ (k,v,vmpop[1].attrs[k],float(v)-float(vmpop[1].attrs[k])) for k,v in ground_error[indexc][2].items() ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the differences in attributes found via brute force versus the genetic algorithm. For the top two candidates.\n",
    "\n",
    "df_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
